{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import dalex as dx\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")\n",
    "import lime.lime_tabular\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import pre-cleaned data, after I divide the dataset in training and testing, 60% and 40% respectively. Everything has to be scaled and I prepare the folds for the 5 cross validation. The steps I am going to follow in wach model are the following:\n",
    "- Recursive feature elimination, I am going to remove the non important features\n",
    "- Grid search, in order to try all the hyperparameters and choose the best ones\n",
    "- Create model with the hyperparameters selected previously and fit it\n",
    "- Make the predictions\n",
    "- Shapley values to understand the model and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\cul_g\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data_info_original = pd.read_csv('info_data.csv')\n",
    "data_info_original = data_info_original.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info = pd.read_csv('info_datav2.csv')\n",
    "data_info = data_info.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_gaussian = data_info[data_info.ANO_FACTURA == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1073878, 27)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data_info_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupData(df, col_groups, ref_col):\n",
    "    testing =  df.groupby(col_groups)[ref_col].sum()\n",
    "    grouped_testing = []\n",
    "    for k,v in zip(testing.index, testing.values):\n",
    "        val = [k[0],k[1],k[2],v,k[3], k[4],k[5],k[6],k[7],k[8],k[9],k[10],k[11],k[12],k[13],k[14],k[15],k[16],k[17],k[18],k[19],k[20],k[21],k[22],k[23],k[24], k[25]]\n",
    "        grouped_testing.append(val)\n",
    "    grouped_testing = pd.DataFrame(grouped_testing, columns = df.columns)\n",
    "    #grouped_testing = grouped_testing.reset_index(drop=True)\n",
    "    return grouped_testing\n",
    "\n",
    "def getISOCountry(iso_country):\n",
    "    indexes = data_info_original[data_info_original['NUMERO_DEUDOR_PAIS_ID'] == iso_country].index\n",
    "    iso_code = data_info.iloc[[indexes[0]]]['NUMERO_DEUDOR_PAIS_ID']\n",
    "    return iso_code.values[0]\n",
    "\n",
    "def getValuesFilter(ds, value, columns, target):\n",
    "    if value == '*': #no filter \n",
    "        return ds\n",
    "    indexes = getIndexFilter(ds, value, target)\n",
    "    datat = getValues(ds, indexes, columns)\n",
    "    return datat\n",
    "\n",
    "def getIndexFilter(dt, value, target):\n",
    "    indexes = dt[dt[target] == value].index\n",
    "    return indexes\n",
    "\n",
    "def getValues(dt, indexes, columns):\n",
    "    datat = []\n",
    "    for k, v in zip(dt.index, dt.values):\n",
    "        if k in indexes:\n",
    "            datat.append(v)\n",
    "    df = pd.DataFrame(datat, columns = columns)\n",
    "    return df\n",
    "\n",
    "def managementDataFile(dat, name_file, r_w):      \n",
    "    # Its important to use binary mode\n",
    "    if r_w == 'r':\n",
    "        dbfile = open(name_file, 'ab')\n",
    "        pickle.dump(dat, dbfile)\n",
    "    elif r_w == 'w':\n",
    "        dbfile = open(name_file, 'rb')\n",
    "        db = pickle.load(dbfile)\n",
    "    dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID', 'PRODUCTO_ID', 'TALLA', 'ESFUERZO_VENTA_ID', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE', 'CONTACTO_SN', 'EDAD_SN', 'GENERO_CONTACTO', 'EDAD_COMPRA', 'EDAD_RANGO_COMPRA', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_grouped = groupData(data_info, columns, 'IMP_VENTA_NETO_EUR')\n",
    "data_info_grouped = data_info_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info_original_grouped = groupData(data_info_original, columns, 'IMP_VENTA_NETO_EUR')\n",
    "data_info_original_grouped = data_info_original_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_spain = getISOCountry('ES') # Get iso from a country you want  to\n",
    "data_info_filtered_grouped = getValuesFilter(data_info_grouped, '*', data_info_grouped.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by country\n",
    "data_info_filtered_grouped = data_info_filtered_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info_original_filtered_grouped = getValuesFilter(data_info_original_grouped, '*', data_info_original_grouped.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by country\n",
    "data_info_original_filtered_grouped = data_info_original_filtered_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by country\n",
    "#iso_spain = getISOCountry('ES') # Get iso from a country you want  to\n",
    "#data_info_filtered = getValuesFilter(data_info, '*', data_info.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by numero deudor pais id\n",
    "#data_info_filtered = data_info_filtered.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "#data_info_original_filtered = getValuesFilter(data_info_original, '*', data_info_original.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by numero deudor pais id\n",
    "#data_info_original_filtered = data_info_original_filtered.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset_original, testdataset_original = train_test_split(data_info_original_filtered_grouped, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "traindataset, testdataset = train_test_split(data_info_filtered_grouped, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "x_train = traindataset.loc[:, traindataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_train = traindataset.loc[:, traindataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "x_test = testdataset.loc[:, testdataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_test = testdataset.loc[:, testdataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindataset_original, testdataset_original = train_test_split(data_info_original_filtered, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "#traindataset, testdataset = train_test_split(data_info_filtered, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "#x_train = traindataset.loc[:, traindataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "#y_train = traindataset.loc[:, traindataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "#x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "#x_test = testdataset.loc[:, testdataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "#y_test = testdataset.loc[:, testdataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "#x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)\n",
    "#x_test = pd.DataFrame(scaler.fit_transform(x_test), columns = x_test.columns)\n",
    "normalizer = MinMaxScaler(feature_range = (-1, 1))\n",
    "x_train = pd.DataFrame(normalizer.fit_transform(x_train), columns= x_train.columns, index = traindataset.index)\n",
    "x_test = pd.DataFrame(normalizer.fit_transform(x_test), columns= x_test.columns, index = testdataset.index)\n",
    "mseresults = pd.DataFrame()\n",
    "timeexecution = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits = 5, shuffle = False) # if shuffle false, random state doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to choose which are the most important features in the model, I will use R2 to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = [{'n_features_to_select': list(range(1, 26))}]\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm)\n",
    "model_cv = GridSearchCV(estimator = rfe, param_grid = hyper_params, scoring= 'r2', cv = folds, verbose = 1, return_train_score=True)      \n",
    "model_cv.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_results)\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,6))\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='lower right')\n",
    "plt.savefig('../Output/testscoretrain.png')\n",
    "plt.show()\n",
    "\n",
    "display(cv_results[['param_n_features_to_select', 'mean_train_score', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_optimal = 25 # model_cv.best_params_['n_features_to_select']\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm, n_features_to_select= n_features_optimal)             \n",
    "rfe = rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I select only the parameters I am interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFeatures(dt_train, dt_test, rfe_model):\n",
    "    chosen = pd.DataFrame(rfe_model.support_, index=dt_train.columns, columns=['Rank'])\n",
    "    featuresnotselected = []\n",
    "    for k, v in zip(chosen.index, chosen.values):\n",
    "        if v == False:\n",
    "            featuresnotselected.append(k)\n",
    "    dt_train_final = dt_train.drop(columns= featuresnotselected)\n",
    "    dt_test_final = dt_test.drop(columns= featuresnotselected)\n",
    "    return dt_train_final, dt_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_LM, x_test_LM = removeFeatures(x_train, x_test, rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I create the model with the best parametrization possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model = LinearRegression()\n",
    "start_time = time.time()\n",
    "lm_model.fit(x_train_LM, y_train)\n",
    "timeexecution['lm'] = (time.time() - start_time)\n",
    "y_pred = lm_model.predict(x_test_LM)\n",
    "results = y_test.copy()\n",
    "results['lm'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         IMP_VENTA_NETO_EUR          lm\n641700               110.31  110.577613\n641701               110.31  111.485371\n641702               110.31  112.607074\n641703                80.38  121.092480\n641704                86.57  102.172864\n...                     ...         ...\n1069495               82.68  154.071804\n1069496               56.81  143.517590\n1069497               56.81  147.638070\n1069498               68.19  244.240802\n1069499               79.20  235.452249\n\n[427800 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMP_VENTA_NETO_EUR</th>\n      <th>lm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>641700</th>\n      <td>110.31</td>\n      <td>110.577613</td>\n    </tr>\n    <tr>\n      <th>641701</th>\n      <td>110.31</td>\n      <td>111.485371</td>\n    </tr>\n    <tr>\n      <th>641702</th>\n      <td>110.31</td>\n      <td>112.607074</td>\n    </tr>\n    <tr>\n      <th>641703</th>\n      <td>80.38</td>\n      <td>121.092480</td>\n    </tr>\n    <tr>\n      <th>641704</th>\n      <td>86.57</td>\n      <td>102.172864</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069495</th>\n      <td>82.68</td>\n      <td>154.071804</td>\n    </tr>\n    <tr>\n      <th>1069496</th>\n      <td>56.81</td>\n      <td>143.517590</td>\n    </tr>\n    <tr>\n      <th>1069497</th>\n      <td>56.81</td>\n      <td>147.638070</td>\n    </tr>\n    <tr>\n      <th>1069498</th>\n      <td>68.19</td>\n      <td>244.240802</td>\n    </tr>\n    <tr>\n      <th>1069499</th>\n      <td>79.20</td>\n      <td>235.452249</td>\n    </tr>\n  </tbody>\n</table>\n<p>427800 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#De moment no ho utilitzar�\n",
    "F-test, p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[('ANO_FACTURA', 4.1434055778222634e-231),\n ('MES_FACTURA', 0.0),\n ('FECHA_FACTURA', 0.0),\n ('TEMPORADA_COMERCIAL_ID', 0.0),\n ('PRODUCTO_ID', 2.3894921765629806e-06),\n ('TALLA', 0.0),\n ('ESFUERZO_VENTA_ID', 0.0),\n ('NUMERO_DEUDOR_PAIS_ID', 0.0),\n ('JERARQUIA_PROD_ID', 0.0),\n ('GRUPO_ARTICULO_PRODUCTO_ID', 0.0),\n ('GENERO_PRODUCTO', 0.0),\n ('CATEGORIA', 0.0),\n ('TIPOLOGIA', 0.0),\n ('CONSUMER_COLOR', 0.0),\n ('CREMALLERA', 0.0),\n ('CORDONES', 0.0),\n ('OUTSOLE_SUELA_TIPO', 0.0),\n ('OUTSOLE_SUELA_SUBTIPO', 0.0),\n ('PLANTILLA_EXTRAIBLE', 0.0),\n ('CONTACTO_SN', 0.530563606061178),\n ('EDAD_SN', 0.0),\n ('GENERO_CONTACTO', 1.336425876962243e-119),\n ('EDAD_COMPRA', 1.953740549724508e-273),\n ('CIUDAD_CONTACTO', 0.0),\n ('IDIOMA_CONTACTO', 1.2182858071684708e-178)]"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[('ANO_FACTURA', 1054.4040344097423),\n ('MES_FACTURA', 13736.605663469945),\n ('FECHA_FACTURA', 2603.4289561131222),\n ('TEMPORADA_COMERCIAL_ID', 5333.720285900557),\n ('PRODUCTO_ID', 22.253691562766942),\n ('TALLA', 43307.2983188973),\n ('ESFUERZO_VENTA_ID', 4150.099866883849),\n ('NUMERO_DEUDOR_PAIS_ID', 9793.690289166436),\n ('JERARQUIA_PROD_ID', 65292.98199596666),\n ('GRUPO_ARTICULO_PRODUCTO_ID', 120259.19322822579),\n ('GENERO_PRODUCTO', 48086.87504027428),\n ('CATEGORIA', 33732.95716462647),\n ('TIPOLOGIA', 16193.592980515188),\n ('CONSUMER_COLOR', 4962.795860924913),\n ('CREMALLERA', 13283.941855771413),\n ('CORDONES', 2026.706006740628),\n ('OUTSOLE_SUELA_TIPO', 4827.620341551089),\n ('OUTSOLE_SUELA_SUBTIPO', 7479.131152984338),\n ('PLANTILLA_EXTRAIBLE', 5197.039935901332),\n ('CONTACTO_SN', 0.39331239868704987),\n ('EDAD_SN', 4564.633761783719),\n ('GENERO_CONTACTO', 540.9154228734118),\n ('EDAD_COMPRA', 1249.5051631407036),\n ('CIUDAD_CONTACTO', 4689.334473132314),\n ('IDIOMA_CONTACTO', 812.6864712591951)]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "f_val, p_val = feature_selection.f_regression(x_train_LM, y_train) #Repassar\n",
    "display(list(zip(x_train_LM.columns, p_val)))\n",
    "print('')\n",
    "display(list(zip(x_train_LM.columns, f_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Absolute Error: 68.98225742153356\nMean Squared Error: 6340.82948686885\nRoot Mean Squared Error: 79.6293255457363\nR2:  -2.05915698685919\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2: ', metrics.r2_score(y_test, y_pred))\n",
    "mseresults['lm'] = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(range(0, 25), lm_model.coef_[0])\n",
    "plt.yticks(range(0, 25), x_train_LM.columns, fontsize = 8)\n",
    "plt.title('Coefficients')\n",
    "plt.savefig('../Output/coefflinear.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_groups = {\n",
    "    'Date': ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID'],\n",
    "    'Product': ['PRODUCTO_ID', 'TALLA', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE'],\n",
    "    'Age': ['EDAD_SN', 'EDAD_COMPRA'],\n",
    "    'ContanctInfo': ['NUMERO_DEUDOR_PAIS_ID', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO', 'GENERO_CONTACTO', 'CONTACTO_SN'],\n",
    "    'SalePerson': ['ESFUERZO_VENTA_ID']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_explainer = dx.Explainer(lm_model, x_test_LM, y_test)\n",
    "explanation = linear_explainer.model_parts()\n",
    "explanation.plot()\n",
    "groupedexpl = linear_explainer.model_parts(variable_groups=variable_groups)\n",
    "groupedexpl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "usa_iso = getISOCountry('US')\n",
    "indexusadata = getIndexFilter(testdataset, usa_iso, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexusadata_original = getIndexFilter(testdataset_original, 'US', 'NUMERO_DEUDOR_PAIS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset_original.loc[indexspaindata_original[0]])\n",
    "display(testdataset_original.loc[indexspaindata_original[20]])\n",
    "display(testdataset_original.loc[indexspaindata_original[200]])\n",
    "display(testdataset_original.loc[indexspaindata_original[500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_henry_es_0 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[0]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_0.plot()\n",
    "bd_henry_es_1 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[20]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_1.plot()\n",
    "bd_henry_es_2 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[200]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_2.plot()\n",
    "bd_henry_es_3 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[500]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset_original.loc[indexusadata_original[0]])\n",
    "display(testdataset_original.loc[indexusadata_original[20]])\n",
    "display(testdataset_original.loc[indexusadata_original[200]])\n",
    "display(testdataset_original.loc[indexusadata_original[500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_henry_us_1 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[0]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_1.plot()\n",
    "bd_henry_us_2 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[20]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_2.plot()\n",
    "bd_henry_us_3 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[200]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_3.plot()\n",
    "bd_henry_us_4 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[500]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, I am going to work with, the parameters max_depth, learning_rate and subsample from the Xgboost model and n_features_to_select from RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=y_train, feature_names=list(x_train.columns))\n",
    "dtest = xgb.DMatrix(x_test, feature_names=list(x_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "#hyper_params = {'estimator__max_depth':[1, 10, 15], 'estimator__n_estimators': [150, 300, 500], 'estimator__learning_rate':[0.1, 0.05, 0.01] }#, 'n_features_to_select': list(range(1, 26))}\n",
    "hyper_params = {'n_features_to_select': list(range(1, 26))}\n",
    "xgbm = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "rfe_xgboost = RFE(xgbm)\n",
    "model_cv_xgb = GridSearchCV(estimator = rfe_xgboost, param_grid = hyper_params, scoring= 'r2', cv = folds, verbose = 1, return_train_score=True)      \n",
    "model_cv_xgb.fit(x_train, y_train)\n",
    "cv_results_xgb = pd.DataFrame(model_cv_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_results_xgb)\n",
    "print(model_cv_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,6))\n",
    "plt.plot(cv_results_xgb[\"param_n_features_to_select\"], cv_results_xgb[\"mean_test_score\"])\n",
    "plt.plot(cv_results_xgb[\"param_n_features_to_select\"], cv_results_xgb[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='lower right')\n",
    "plt.savefig('../Output/testscoretrain_xgb.png')\n",
    "plt.show()\n",
    "\n",
    "display(cv_results_xgb[['param_n_features_to_select', 'mean_train_score', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I get the best parametrization, I execute the model with it, by the moment this is an example because I haven't execute the previous, too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_optimal = 21 # model_cv.best_params_['n_features_to_select']\n",
    "xgbm = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "rfe = RFE(xgbm, n_features_to_select= n_features_optimal)             \n",
    "rfe = rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_XGBM, x_test_XGBM = removeFeatures(x_train, x_test, rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Featues chosen: '"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'ANO_FACTURA',\n 'CATEGORIA',\n 'CIUDAD_CONTACTO',\n 'CONSUMER_COLOR',\n 'CORDONES',\n 'CREMALLERA',\n 'EDAD_COMPRA',\n 'ESFUERZO_VENTA_ID',\n 'FECHA_FACTURA',\n 'GENERO_PRODUCTO',\n 'IDIOMA_CONTACTO',\n 'JERARQUIA_PROD_ID',\n 'MES_FACTURA',\n 'NUMERO_DEUDOR_PAIS_ID',\n 'OUTSOLE_SUELA_SUBTIPO',\n 'OUTSOLE_SUELA_TIPO',\n 'PLANTILLA_EXTRAIBLE',\n 'PRODUCTO_ID',\n 'TALLA',\n 'TEMPORADA_COMERCIAL_ID',\n 'TIPOLOGIA'}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Features discarted: '"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'CONTACTO_SN', 'EDAD_SN', 'GENERO_CONTACTO', 'GRUPO_ARTICULO_PRODUCTO_ID'}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display('Featues chosen: ', set(x_train_XGBM.columns))\n",
    "display('Features discarted: ',set(x_train.columns) - set(x_train_XGBM.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "start_time = time.time()\n",
    "xgbm.fit(x_train_XGBM, y_train)\n",
    "timeexecution['xgboost'] = (time.time() - start_time)\n",
    "y_pred = xgbm.predict(x_test_XGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = y_test.copy()\n",
    "results['xgboost'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Absolute Error: 21.714228681888905\nMean Squared Error: 901.8878491721711\nRoot Mean Squared Error: 30.031447670270094\nR2: 0.564880821843251\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))\n",
    "mseresults['xgboost'] = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I create an explainer for the model, the inputs are the model, x_train and y_train. After, i can get the variable importance and the reverse cumulative distribution of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_groups = {\n",
    "    'Date': ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID'],\n",
    "    'Product': ['PRODUCTO_ID', 'TALLA', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE'],\n",
    "    'Age': ['EDAD_COMPRA'],\n",
    "    'ContanctInfo': ['NUMERO_DEUDOR_PAIS_ID', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO'],\n",
    "    'SalePerson': ['ESFUERZO_VENTA_ID']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 427800 rows 21 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.DataFrame. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 427800 values\n",
      "  -> model_class       : xgboost.sklearn.XGBRegressor (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_default at 0x000002B58D904E50> will be used (default)\n",
      "  -> predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n",
      "  -> predicted values  : min = 29.2, mean = 1.05e+02, max = 3.2e+02\n",
      "  -> model type        : regression will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = -1.5e+02, mean = 2.31, max = 7.01e+02\n",
      "  -> model_info        : package xgboost\n",
      "\n",
      "A new explainer has been created!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "base": 26.606600602996686,
         "hoverinfo": "text",
         "hoverlabel": {
          "bgcolor": "rgba(0,0,0,0.8)"
         },
         "hovertext": [
          "Model: XGBRegressor loss after<br>variable: PRODUCTO_ID is permuted: 30.791<br>Drop-out loss change: +4.185",
          "Model: XGBRegressor loss after<br>variable: CATEGORIA is permuted: 29.859<br>Drop-out loss change: +3.253",
          "Model: XGBRegressor loss after<br>variable: JERARQUIA_PROD_ID is permuted: 29.652<br>Drop-out loss change: +3.046",
          "Model: XGBRegressor loss after<br>variable: NUMERO_DEUDOR_PAIS_ID is permuted: 29.038<br>Drop-out loss change: +2.432",
          "Model: XGBRegressor loss after<br>variable: TIPOLOGIA is permuted: 28.759<br>Drop-out loss change: +2.152",
          "Model: XGBRegressor loss after<br>variable: CONSUMER_COLOR is permuted: 27.395<br>Drop-out loss change: +0.788",
          "Model: XGBRegressor loss after<br>variable: CIUDAD_CONTACTO is permuted: 27.293<br>Drop-out loss change: +0.687",
          "Model: XGBRegressor loss after<br>variable: CREMALLERA is permuted: 27.122<br>Drop-out loss change: +0.515",
          "Model: XGBRegressor loss after<br>variable: PLANTILLA_EXTRAIBLE is permuted: 27.01<br>Drop-out loss change: +0.403",
          "Model: XGBRegressor loss after<br>variable: CORDONES is permuted: 27.001<br>Drop-out loss change: +0.394"
         ],
         "marker": {
          "color": "#46bac2"
         },
         "orientation": "h",
         "showlegend": false,
         "text": [
          "+4.185",
          "+3.253",
          "+3.046",
          "+2.432",
          "+2.152",
          "+0.788",
          "+0.687",
          "+0.515",
          "+0.403",
          "+0.394"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          4.184534602568107,
          3.2526593904381613,
          3.0457000525362368,
          2.4318354260373027,
          2.15217186062079,
          0.7882119501576881,
          0.686846966318857,
          0.515369108339268,
          0.40316869018033685,
          0.3944407863838002
         ],
         "xaxis": "x",
         "y": [
          "PRODUCTO_ID",
          "CATEGORIA",
          "JERARQUIA_PROD_ID",
          "NUMERO_DEUDOR_PAIS_ID",
          "TIPOLOGIA",
          "CONSUMER_COLOR",
          "CIUDAD_CONTACTO",
          "CREMALLERA",
          "PLANTILLA_EXTRAIBLE",
          "CORDONES"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "XGBRegressor",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "drop-out loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 383,
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 26.606600602996686,
          "x1": 26.606600602996686,
          "xref": "x",
          "y0": -1,
          "y1": 10,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Variable Importance",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          25.978310918188114,
          31.41889489522262
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "type": "category"
        }
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "base": 26.606600602996686,
         "hoverinfo": "text",
         "hoverlabel": {
          "bgcolor": "rgba(0,0,0,0.8)"
         },
         "hovertext": [
          "Model: XGBRegressor loss after<br>variable: Product is permuted: 35.48<br>Drop-out loss change: +8.873",
          "Model: XGBRegressor loss after<br>variable: ContanctInfo is permuted: 29.474<br>Drop-out loss change: +2.867",
          "Model: XGBRegressor loss after<br>variable: Age is permuted: 26.8<br>Drop-out loss change: +0.193",
          "Model: XGBRegressor loss after<br>variable: Date is permuted: 26.606<br>Drop-out loss change: -0.0",
          "Model: XGBRegressor loss after<br>variable: SalePerson is permuted: 26.606<br>Drop-out loss change: -0.001"
         ],
         "marker": {
          "color": "#46bac2"
         },
         "orientation": "h",
         "showlegend": false,
         "text": [
          "+8.873",
          "+2.867",
          "+0.193",
          "0.0",
          "0.001"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          8.873431677935116,
          2.866912383459052,
          0.19329009700272692,
          -0.00047474066142427773,
          -0.0007973736598785308
         ],
         "xaxis": "x",
         "y": [
          "Product",
          "ContanctInfo",
          "Age",
          "Date",
          "SalePerson"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "XGBRegressor",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "drop-out loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 283,
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 26.606600602996686,
          "x1": 26.606600602996686,
          "xref": "x",
          "y0": -1,
          "y1": 5,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Variable Importance",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          25.27466887159756,
          36.811166638671054
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "type": "category"
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "xgboost_explainer = dx.Explainer(xgbm, x_test_XGBM, y_test)\n",
    "explanation = xgboost_explainer.model_parts()\n",
    "explanation.plot()\n",
    "res = xgboost_explainer.model_performance(model_type='regression')\n",
    "res.plot()\n",
    "groupedexpl = xgboost_explainer.model_parts(variable_groups=variable_groups)\n",
    "groupedexpl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                     mse       rmse        r2        mae        mad\nXGBRegressor  901.887849  30.031448  0.564881  21.714229  16.755986\n"
     ]
    }
   ],
   "source": [
    "print(xgboost_explainer.model_performance().result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I can group the features into a new ones. In this case, I have created different variables that include a similar meaning.\n",
    "- Date: ANO_FACTURA, MES_FACTURA, FECHA_FACTURA, TEMPORADA_COMERCIAL_ID\n",
    "- Product: PRODUCTO_ID, TALLA, GRUPO_ARTICULO_PRODUCTO_ID, GENERO_PRODUCTO, CATEGORIA, TIPOLOGIA, CONSUMER_COLOR, CREMALLERA, CORDONES, OUTSOLE_SUELA_TIPO, OUTSOLE_SUELA_SUBTIPO, PLANTILLA_EXTRAIBLE\n",
    "- Age: EDAD_SN, EDAD_COMPRA\n",
    "- ContanctInfo: NUMERO_DEUDOR_PAIS_ID, CIUDAD_CONTACTO, IDIOMA_CONTACTO, GENERO_CONTACTO, CONTACTO_SN\n",
    "- SalePerson: ESFUERZO_VENTA_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'xgb_sh_es_0' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0ba6d599d2f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mxgb_sh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_parts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_XGBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexspaindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'shap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxgb_sh_es_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mxgb_sh_es_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_parts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_XGBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexspaindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'shap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb_sh_es_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxgb_sh_es_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_parts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_XGBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexspaindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'shap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_sh_es_0' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_sh_es_0 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[0]]]), type = 'shap')\n",
    "xgb_sh_es_0.plot()\n",
    "xgb_sh_es_1 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[20]]]), type = 'shap')\n",
    "xgb_sh_es_1.plot()\n",
    "xgb_sh_es_2 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[200]]]), type = 'shap')\n",
    "xgb_sh_es_2.plot()\n",
    "xgb_sh_es_3 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[500]]]), type = 'shap')\n",
    "xgb_sh_es_3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sh_us_0 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[0]]]), type = 'shap')\n",
    "xgb_sh_us_0.plot()\n",
    "xgb_sh_us_1 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[20]]]), type = 'shap')\n",
    "xgb_sh_us_1.plot()\n",
    "xgb_sh_us_2 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[200]]]), type = 'shap')\n",
    "xgb_sh_us_2.plot()\n",
    "xgb_sh_us_3 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[500]]]), type = 'shap')\n",
    "xgb_sh_us_3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,6))\n",
    "#chosen = pd.DataFrame(rfe_xgboost.support_, index=x_train.columns, columns=['Rank'])\n",
    "#featuresnotselected = []\n",
    "#for k, v in zip(chosen.index, chosen.values):\n",
    "#    if v == False:\n",
    "#        featuresnotselected.append(k)\n",
    "#x_train_final = x_train.drop(columns= featuresnotselected)\n",
    "#x_test_final = x_test.drop(columns= featuresnotselected)\n",
    "\n",
    "#plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "#plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "#plt.xlabel('number of features')\n",
    "#plt.ylabel('r-squared')\n",
    "#plt.title(\"Optimal Number of Features\")\n",
    "#plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbour regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val = [] #to store rmse values for different k\n",
    "mse_val = []\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors = 7)\n",
    "start_time = time.time()\n",
    "model.fit(x_train, y_train)  #fit the model\n",
    "timeexecution['knn'] = (time.time() - start_time)\n",
    "y_pred=model.predict(x_test) #make prediction on test set\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "error = np.sqrt(metrics.mean_squared_error(y_test, y_pred)) #calculate rmse\n",
    "rmse_val.append(error) #store rmse values\n",
    "mse_val.append(mse)\n",
    "results['knn'] = y_pred\n",
    "mseresults['knn'] = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curvermse = pd.DataFrame(rmse_val) #elbow curve \n",
    "#curvermse.plot()\n",
    "#curvemse = pd.DataFrame(mse_val)\n",
    "#curvemse.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_reg = MLPRegressor(hidden_layer_sizes=(300, 300),  activation='logistic', solver='adam', alpha=0.01, batch_size='auto', learning_rate='constant', learning_rate_init=0.01, max_iter=1000, shuffle=False, tol=0.0001, verbose=True, early_stopping= True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn_reg.fit(x_train, np.ravel(y_train))\n",
    "y_pred = nn_reg.predict(x_test)\n",
    "nn_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg_pred = lambda x: nn_reg.predict(x).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nn'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "print(indexspaindata)\n",
    "print(indexspaindata_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset.loc[indexspaindata[0]])\n",
    "display(testdataset_original.loc[indexspaindata_original[0]])\n",
    "display(testdataset_original[(testdataset_original['PRODUCTO_ID'] == 10189) & (testdataset_original['NUMERO_DEUDOR_PAIS_ID'] == 'ES') & (test)].index)\n",
    "display(testdataset_original.loc[[644943]])\n",
    "i=0\n",
    "for v in indexspaindata_original:\n",
    "    if v == 644943:\n",
    "        break\n",
    "    else:\n",
    "        i = i+1\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train.values, feature_names=x_train.columns, verbose=True, mode='regression')\n",
    "exp = explainer.explain_instance(np.ravel(x_test.iloc[[indexspaindata[0]]]), nn_reg_pred, num_features=10)\n",
    "exp.show_in_notebook(show_table=True)\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset.iloc[[100]])\n",
    "display(y_pred[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(39931191)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 5) #fully connected 1\n",
    "        self.fc = nn.Linear(5, num_classes) #fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach())) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Densetarget.squeeze(1)\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000 #1000 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "\n",
    "input_size = 25 #number of features\n",
    "hidden_size = 20 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes\n",
    "\n",
    "x_train_tensors = Variable(torch.Tensor(x_train.values))\n",
    "x_test_tensors = Variable(torch.Tensor(x_test.values))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test.values))\n",
    "\n",
    "x_train_tensors_final = torch.reshape(x_train_tensors,   (x_train_tensors.shape[0], 1, x_train_tensors.shape[1]))\n",
    "x_test_tensors_final = torch.reshape(x_test_tensors,  (x_test_tensors.shape[0], 1, x_test_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM(num_classes, input_size, hidden_size, num_layers, x_train_tensors_final.shape[1]) #our lstm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  outputs = lstm_model.forward(x_train_tensors_final) #forward pass\n",
    "  optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "  # obtain the loss function\n",
    "  loss = criterion(outputs, y_train_tensors)\n",
    "  loss.backward() #calculates the loss of the loss function\n",
    "  optimizer.step() #improve from loss, i.e backprop\n",
    "  if epoch % 100 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = lstm1(x_test_tensors_final)#forward pass\n",
    "data_predict = train_predict.data.numpy() #numpy conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictplot(dso, dsp, dso_size, range_plot):\n",
    "    plt.axvline(x=dso_size, c='r', linestyle='--') #size of the training set\n",
    "    plt.scatter(range(0, range_plot),y_test[:range_plot], label='Actual Data') #actual plot\n",
    "    plt.scatter(range(0, range_plot), data_predict[:range_plot], label='Predicted Data') #predicted plot\n",
    "    plt.title('Time-Series Prediction')\n",
    "    plt.legend()\n",
    "    plt.savefig('../Output/timeseriespredictionLSTM.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, data_predict))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, data_predict))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, data_predict)))\n",
    "print('R2:', metrics.r2_score(y_test, data_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(lstm1)\n",
    "t = torch.Tensor(x_test.iloc[[20]].values)\n",
    "x_test_tensors_final = torch.reshape(x_test_tensors,  (x_test_tensors.shape[0], 1, x_test_tensors.shape[1]))\n",
    "x_test_tensors_final_ig = torch.reshape(x_test_tensors_final[0],  (1, 1, 25))\n",
    "attribution = ig.attribute(x_test_tensors_final_ig)\n",
    "x_test_tensors_final_ig\n",
    "display(attribution.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importance(title, features, importance, xtitle):\n",
    "    plt.barh(range(0, features.size), importance, align = 'center')\n",
    "    plt.yticks(range(0, features.size), features, wrap = True)\n",
    "    plt.ylabel(xtitle)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importance('Variable Importance', x_test.columns, attribution.detach().numpy()[0][0], 'Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LayerConductance(lstm1.forward, layer, device_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n",
    "\n",
    "#knn = neighbors.KNeighborsRegressor()\n",
    "\n",
    "#model = GridSearchCV(knn, params, cv=10)\n",
    "#model.fit(x_train,y_train)\n",
    "#model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gaussian_filtered = getValuesFilter(data_gaussian, 14, data_gaussian.columns) # iso = * -> no filter by numero deudor pais id\n",
    "data_gaussian_filtered = data_gaussian_filtered.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "traindataset, testdataset = train_test_split(data_gaussian, test_size=0.01, shuffle= False)\n",
    "traindataset, testdataset = train_test_split(testdataset, test_size=0.01, shuffle= False)\n",
    "x_train = traindataset.loc[:, traindataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_train = traindataset.loc[:, traindataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "x_test = testdataset.loc[:, testdataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_test = testdataset.loc[:, testdataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)\n",
    "x_test = pd.DataFrame(scaler.fit_transform(x_test), columns = x_test.columns)\n",
    "traindataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# massive dataset, try with something smaller\n",
    "kernel = kernel = RBF(10, (1e-2, 1e2)) # Check which ones choose\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gpr.fit(x_train, y_train)\n",
    "gpr.score(x_train, y_train)\n",
    "y_pred, sigma = gpr.predict(x_test, return_std=True)\n",
    "results['gaussianP'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
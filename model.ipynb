{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic('re-sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import dalex as dx\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")\n",
    "import lime.lime_tabular\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from LSTM import LSTM\n",
    "from captum.attr import NeuronConductance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import pre-cleaned data, after I divide the dataset in training and testing, 60% and 40% respectively. Everything has to be scaled and I prepare the folds for the 5 cross validation. The steps I am going to follow in wach model are the following:\n",
    "- Recursive feature elimination, I am going to remove the non important features\n",
    "- Grid search, in order to try all the hyperparameters and choose the best ones\n",
    "- Create model with the hyperparameters selected previously and fit it\n",
    "- Make the predictions\n",
    "- Shapley values to understand the model and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_original = pd.read_csv('info_data.csv')\n",
    "data_info_original = data_info_original.drop(columns = ['DAY_OF_WEEK'])\n",
    "data_info_original = data_info_original.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info = pd.read_csv('info_datav2.csv')\n",
    "data_info = data_info.drop(columns = ['DAY_OF_WEEK'])\n",
    "data_info = data_info.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_gaussian = data_info[data_info.ANO_FACTURA == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_info_original[(data_info_original['PRODUCTO_ID'] == 'K400381-001') & (data_info_original['OUTSOLE_SUELA_SUBTIPO'] != 'HIGH')]['OUTSOLE_SUELA_SUBTIPO'])\n",
    "display(data_info_original[(data_info_original['PRODUCTO_ID'] == 'K400381-001') & (data_info_original['PLANTILLA_EXTRAIBLE'] != 'NO')]['PLANTILLA_EXTRAIBLE'])\n",
    "display(data_info_original[(data_info_original['PRODUCTO_ID'] == 'K400381-001') & (data_info_original['CONSUMER_COLOR'] != 'Black')]['CONSUMER_COLOR']) # ALWAYS BROWN\n",
    "display(data_info_original[(data_info_original['PRODUCTO_ID'] == 'K400381-001') & (data_info_original['CREMALLERA'] != 'SI')]['CREMALLERA']) #Always Orxford\n",
    "#display(data_info_original[(data_info_original['PRODUCTO_ID'] == '16002-194')]['PRODUCTO_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_info.shape)\n",
    "print(data_info_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupData(df, col_groups, ref_col):\n",
    "    testing =  df.groupby(col_groups)[ref_col].sum()\n",
    "    grouped_testing = []\n",
    "    for k,v in zip(testing.index, testing.values):\n",
    "        val = [k[0],k[1],k[2],v,k[3], k[4],k[5],k[6],k[7],k[8],k[9],k[10],k[11],k[12],k[13],k[14],k[15],k[16],k[17],k[18],k[19],k[20],k[21],k[22],k[23],k[24], k[25]]\n",
    "        grouped_testing.append(val)\n",
    "    grouped_testing = pd.DataFrame(grouped_testing, columns = df.columns)\n",
    "    #grouped_testing = grouped_testing.reset_index(drop=True)\n",
    "    return grouped_testing\n",
    "\n",
    "def getISOCountry(iso_country):\n",
    "    indexes = data_info_original[data_info_original['NUMERO_DEUDOR_PAIS_ID'] == iso_country].index\n",
    "    iso_code = data_info.iloc[[indexes[0]]]['NUMERO_DEUDOR_PAIS_ID']\n",
    "    return iso_code.values[0]\n",
    "\n",
    "def getEncode(dto, dt, val, col):\n",
    "    indexes = dto[-dto[col] == val].index\n",
    "    code = dt.iloc[[indexes[0]]][col]\n",
    "    return code.values[0]\n",
    "\n",
    "def getValuesFilter(ds, value, columns, target):\n",
    "    if value == '*': #no filter \n",
    "        return ds\n",
    "    indexes = getIndexFilter(ds, value, target)\n",
    "    datat = getValues(ds, indexes, columns)\n",
    "    return datat\n",
    "\n",
    "def getIndexFilter(dt, value, target):\n",
    "    indexes = dt[dt[target] == value].index\n",
    "    return indexes\n",
    "\n",
    "def getValues(dt, indexes, columns):\n",
    "    datat = []\n",
    "    for k, v in zip(dt.index, dt.values):\n",
    "        if k in indexes:\n",
    "            datat.append(v)\n",
    "    df = pd.DataFrame(datat, columns = columns)\n",
    "    return df\n",
    "\n",
    "def saveModelToFile(dat, name_file):\n",
    "    dbfile = open('./Models/'+name_file, 'ab')\n",
    "    pickle.dump(dat, dbfile)\n",
    "\n",
    "def loadModelFromFile(name_file):\n",
    "    dbfile = open('./Models/'+name_file, 'rb')\n",
    "    md = pickle.load(dbfile)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID', 'PRODUCTO_ID', 'TALLA', 'ESFUERZO_VENTA_ID', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE', 'CONTACTO_SN', 'EDAD_SN', 'GENERO_CONTACTO', 'EDAD_COMPRA', 'EDAD_RANGO_COMPRA', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_grouped = groupData(data_info, columns, 'IMP_VENTA_NETO_EUR')\n",
    "data_info_grouped = data_info_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info_original_grouped = groupData(data_info_original, columns, 'IMP_VENTA_NETO_EUR')\n",
    "data_info_original_grouped = data_info_original_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_info_grouped.shape)\n",
    "print(data_info_original_grouped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_filtered_grouped = getValuesFilter(data_info_grouped, '*', data_info_grouped.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by country\n",
    "data_info_filtered_grouped = data_info_filtered_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "data_info_original_filtered_grouped = getValuesFilter(data_info_original_grouped, '*', data_info_original_grouped.columns, 'NUMERO_DEUDOR_PAIS_ID') # iso = * -> no filter by country\n",
    "data_info_original_filtered_grouped = data_info_original_filtered_grouped.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset_original, testdataset_original = train_test_split(data_info_original_filtered_grouped, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "traindataset, testdataset = train_test_split(data_info_filtered_grouped, test_size=0.4, shuffle= False) # To use all the data, change to -> data_info\n",
    "x_train = traindataset.loc[:, traindataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_train = traindataset.loc[:, traindataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "x_test = testdataset.loc[:, testdataset.columns != 'IMP_VENTA_NETO_EUR']\n",
    "y_test = testdataset.loc[:, testdataset.columns == 'IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: 2018 and 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_info_filtered_grouped[data_info_filtered_grouped['ANO_FACTURA'] == 2018]\n",
    "y_train = x_train['IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.loc[:, x_train.columns != 'IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "x_test = data_info_filtered_grouped[data_info_filtered_grouped['ANO_FACTURA'] == 2019]\n",
    "y_test = x_test['IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.loc[:, x_test.columns != 'IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: Rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_info_filtered_grouped[(data_info_filtered_grouped['ANO_FACTURA'] == 2020) & (data_info_filtered_grouped['MES_FACTURA'] < 10)]\n",
    "y_train = x_train['IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.loc[:, x_train.columns != 'IMP_VENTA_NETO_EUR']\n",
    "x_train = x_train.drop(columns=['EDAD_RANGO_COMPRA'])\n",
    "x_test = data_info_filtered_grouped[(data_info_filtered_grouped['ANO_FACTURA'] == 2020) & (data_info_filtered_grouped['MES_FACTURA'] > 9)]\n",
    "y_test = x_test['IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.loc[:, x_test.columns != 'IMP_VENTA_NETO_EUR']\n",
    "x_test = x_test.drop(columns = 'EDAD_RANGO_COMPRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler(feature_range = (-1, 1))\n",
    "x_train = pd.DataFrame(normalizer.fit_transform(x_train), columns= x_train.columns, index = x_train.index)\n",
    "x_test = pd.DataFrame(normalizer.fit_transform(x_test), columns= x_test.columns, index = x_test.index)\n",
    "mseresults = pd.DataFrame()\n",
    "timeexecution = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits = 5, shuffle = False) # if shuffle false, random state doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to choose which are the most important features in the model, I will use R2 to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = [{'n_features_to_select': list(range(1, 26))}]\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm)\n",
    "model_cv = GridSearchCV(estimator = rfe, param_grid = hyper_params, scoring= 'r2', cv = folds, verbose = 1, return_train_score=True)      \n",
    "model_cv.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_results)\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,6))\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='lower right')\n",
    "plt.savefig('../Output/testscoretrain.png')\n",
    "plt.show()\n",
    "\n",
    "display(cv_results[['param_n_features_to_select', 'mean_train_score', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_optimal = 25 # model_cv.best_params_['n_features_to_select']\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm, n_features_to_select= n_features_optimal)             \n",
    "rfe = rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I select only the parameters I am interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFeatures(dt_train, dt_test, rfe_model):\n",
    "    chosen = pd.DataFrame(rfe_model.support_, index=dt_train.columns, columns=['Rank'])\n",
    "    featuresnotselected = []\n",
    "    for k, v in zip(chosen.index, chosen.values):\n",
    "        if v == False:\n",
    "            featuresnotselected.append(k)\n",
    "    dt_train_final = dt_train.drop(columns= featuresnotselected)\n",
    "    dt_test_final = dt_test.drop(columns= featuresnotselected)\n",
    "    return dt_train_final, dt_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_LM, x_test_LM = removeFeatures(x_train, x_test, rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I create the model with the best parametrization possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model = LinearRegression()\n",
    "start_time = time.time()\n",
    "lm_model.fit(x_train_LM, y_train)\n",
    "timeexecution['lm'] = (time.time() - start_time)\n",
    "y_pred = lm_model.predict(x_test_LM)\n",
    "results = pd.DataFrame(index= testdataset.index, columns = ['Import'])\n",
    "#results = pd.DataFrame()\n",
    "results['Import'] = y_test.copy()\n",
    "results['lm'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModelToFile(lm_model, 'LinearModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#De moment no ho utilitzar�\n",
    "F-test, p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, p_val = feature_selection.f_regression(x_train_LM, y_train) #Repassar\n",
    "display(list(zip(x_train_LM.columns, p_val)))\n",
    "print('')\n",
    "display(list(zip(x_train_LM.columns, f_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2: ', metrics.r2_score(y_test, y_pred))\n",
    "mseresults['lm'] = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(range(0, 25), lm_model.coef_[0])\n",
    "plt.yticks(range(0, 25), x_train_LM.columns, fontsize = 8)\n",
    "plt.title('Coefficients')\n",
    "plt.savefig('../Output/coefflinear.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_groups = {\n",
    "    'Date': ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID'],\n",
    "    'Product': ['PRODUCTO_ID', 'TALLA', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE'],\n",
    "    'Age': ['EDAD_SN', 'EDAD_COMPRA'],\n",
    "    'ContanctInfo': ['NUMERO_DEUDOR_PAIS_ID', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO', 'GENERO_CONTACTO', 'CONTACTO_SN'],\n",
    "    'SalePerson': ['ESFUERZO_VENTA_ID']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_explainer = dx.Explainer(lm_model, x_test_LM, y_test)\n",
    "explanation = linear_explainer.model_parts()\n",
    "explanation.plot()\n",
    "groupedexpl = linear_explainer.model_parts(variable_groups=variable_groups)\n",
    "groupedexpl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "usa_iso = getISOCountry('US')\n",
    "indexusadata = getIndexFilter(testdataset, usa_iso, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexusadata_original = getIndexFilter(testdataset_original, 'US', 'NUMERO_DEUDOR_PAIS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset_original.loc[indexspaindata_original[0]])\n",
    "display(testdataset_original.loc[indexspaindata_original[20]])\n",
    "display(testdataset_original.loc[indexspaindata_original[200]])\n",
    "display(testdataset_original.loc[indexspaindata_original[500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_henry_es_0 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[0]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_0.plot()\n",
    "bd_henry_es_1 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[20]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_1.plot()\n",
    "bd_henry_es_2 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[200]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_2.plot()\n",
    "bd_henry_es_3 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexspaindata[500]]]), type = 'shap', B= 100)\n",
    "bd_henry_es_3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset_original.loc[indexusadata_original[0]])\n",
    "display(testdataset_original.loc[indexusadata_original[20]])\n",
    "display(testdataset_original.loc[indexusadata_original[200]])\n",
    "display(testdataset_original.loc[indexusadata_original[500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_henry_us_1 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[0]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_1.plot()\n",
    "bd_henry_us_2 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[20]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_2.plot()\n",
    "bd_henry_us_3 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[200]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_3.plot()\n",
    "bd_henry_us_4 = linear_explainer.predict_parts(np.ravel(x_test_LM.loc[[indexusadata[500]]]), type = 'shap', B= 100)\n",
    "bd_henry_us_4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, I am going to work with, the parameters max_depth, learning_rate and subsample from the Xgboost model and n_features_to_select from RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=y_train, feature_names=list(x_train.columns))\n",
    "dtest = xgb.DMatrix(x_test, feature_names=list(x_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyper_params = {'estimator__max_depth':[1, 10, 15], 'estimator__n_estimators': [150, 300, 500], 'estimator__learning_rate':[0.1, 0.05, 0.01] }#, 'n_features_to_select': list(range(1, 26))}\n",
    "hyper_params = {'n_features_to_select': list(range(16, 26))}\n",
    "xgbm = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "rfe_xgboost = RFE(xgbm)\n",
    "model_cv_xgb = GridSearchCV(estimator = rfe_xgboost, param_grid = hyper_params, scoring= 'r2', cv = folds, verbose = 1, return_train_score=True)      \n",
    "model_cv_xgb.fit(x_train, y_train)\n",
    "cv_results_xgb = pd.DataFrame(model_cv_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_results_xgb)\n",
    "print(model_cv_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,6))\n",
    "plt.plot(cv_results_xgb[\"param_n_features_to_select\"], cv_results_xgb[\"mean_test_score\"])\n",
    "plt.plot(cv_results_xgb[\"param_n_features_to_select\"], cv_results_xgb[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='lower right')\n",
    "plt.savefig('../Output/testscoretrain_xgb.png')\n",
    "plt.show()\n",
    "\n",
    "display(cv_results_xgb[['param_n_features_to_select', 'mean_train_score', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I get the best parametrization, I execute the model with it, by the moment this is an example because I haven't execute the previous, too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_optimal = 21 # model_cv.best_params_['n_features_to_select']\n",
    "xgbm = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "rfe = RFE(xgbm, n_features_to_select= n_features_optimal)             \n",
    "rfe = rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_XGBM, x_test_XGBM = removeFeatures(x_train, x_test, rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('Featues chosen: ', set(x_train_XGBM.columns))\n",
    "display('Features discarted: ',set(x_train.columns) - set(x_train_XGBM.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(learning_rate =0.01, n_estimators=215, max_depth=10, min_child_weight=0.8, subsample=1, nthread=4)\n",
    "xgb_model.fit(x_train_XGBM, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModelToFile(xgb_model, 'XGBoostModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute it to avoid all the process\n",
    "xgb_model = loadModelFromFile('XGBoostModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(x_test_XGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = y_test.copy()\n",
    "results['xgboost'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))\n",
    "mseresults['xgboost'] = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I create an explainer for the model, the inputs are the model, x_train and y_train. After, i can get the variable importance and the reverse cumulative distribution of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_groups = {\n",
    "    'Date': ['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA', 'TEMPORADA_COMERCIAL_ID'],\n",
    "    'Product': ['PRODUCTO_ID', 'TALLA', 'GENERO_PRODUCTO', 'CATEGORIA', 'TIPOLOGIA', 'CONSUMER_COLOR', 'CREMALLERA', 'CORDONES', 'OUTSOLE_SUELA_TIPO', 'OUTSOLE_SUELA_SUBTIPO', 'PLANTILLA_EXTRAIBLE'],\n",
    "    'Age': ['EDAD_COMPRA'],\n",
    "    'ContanctInfo': ['NUMERO_DEUDOR_PAIS_ID', 'CIUDAD_CONTACTO', 'IDIOMA_CONTACTO'],\n",
    "    'SalePerson': ['ESFUERZO_VENTA_ID']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_explainer = dx.Explainer(xgb_model, x_test_XGBM, y_test)\n",
    "explanation = xgboost_explainer.model_parts()\n",
    "explanation.plot()\n",
    "res = xgboost_explainer.model_performance(model_type='regression')\n",
    "res.plot()\n",
    "groupedexpl = xgboost_explainer.model_parts(variable_groups=variable_groups)\n",
    "groupedexpl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgboost_explainer.model_performance().result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I can group the features into a new ones. In this case, I have created different variables that include a similar meaning.\n",
    "- Date: ANO_FACTURA, MES_FACTURA, FECHA_FACTURA, TEMPORADA_COMERCIAL_ID\n",
    "- Product: PRODUCTO_ID, TALLA, GRUPO_ARTICULO_PRODUCTO_ID, GENERO_PRODUCTO, CATEGORIA, TIPOLOGIA, CONSUMER_COLOR, CREMALLERA, CORDONES, OUTSOLE_SUELA_TIPO, OUTSOLE_SUELA_SUBTIPO, PLANTILLA_EXTRAIBLE\n",
    "- Age: EDAD_SN, EDAD_COMPRA\n",
    "- ContanctInfo: NUMERO_DEUDOR_PAIS_ID, CIUDAD_CONTACTO, IDIOMA_CONTACTO, GENERO_CONTACTO, CONTACTO_SN\n",
    "- SalePerson: ESFUERZO_VENTA_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sh_es_0 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[0]]]), type = 'shap')\n",
    "xgb_sh_es_0.plot()\n",
    "xgb_sh_es_1 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[20]]]), type = 'shap')\n",
    "xgb_sh_es_1.plot()\n",
    "xgb_sh_es_2 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[200]]]), type = 'shap')\n",
    "xgb_sh_es_2.plot()\n",
    "xgb_sh_es_3 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexspaindata[500]]]), type = 'shap')\n",
    "xgb_sh_es_3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sh_us_0 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[0]]]), type = 'shap')\n",
    "xgb_sh_us_0.plot()\n",
    "xgb_sh_us_1 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[20]]]), type = 'shap')\n",
    "xgb_sh_us_1.plot()\n",
    "xgb_sh_us_2 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[200]]]), type = 'shap')\n",
    "xgb_sh_us_2.plot()\n",
    "xgb_sh_us_3 = xgboost_explainer.predict_parts(np.ravel(x_test_XGBM.loc[[indexusadata[500]]]), type = 'shap')\n",
    "xgb_sh_us_3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_reg = MLPRegressor(hidden_layer_sizes=(300, 300),  activation='logistic', solver='adam', alpha=0.01, batch_size='auto', learning_rate='constant', learning_rate_init=0.01, max_iter=1000, shuffle=False, tol=0.0001, verbose=True, early_stopping= True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn_reg.fit(x_train, np.ravel(y_train))\n",
    "saveModelToFile(nn_reg, 'MLPRegressorModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg = loadModelFromFile('MLPRegressorModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn_reg.predict(x_test)\n",
    "nn_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg_pred = lambda x: nn_reg.predict(x).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nn'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train.values, feature_names=x_train.columns, verbose= False, mode='regression')\n",
    "expsp1 = explainer.explain_instance(np.ravel(x_test.loc[[indexspaindata[0]]]), nn_reg_pred, num_features=10)\n",
    "expsp2 = explainer.explain_instance(np.ravel(x_test.loc[[indexspaindata[20]]]), nn_reg_pred, num_features=10)\n",
    "expsp3 = explainer.explain_instance(np.ravel(x_test.loc[[indexspaindata[200]]]), nn_reg_pred, num_features=10)\n",
    "expsp4 = explainer.explain_instance(np.ravel(x_test.loc[[indexspaindata[500]]]), nn_reg_pred, num_features=10)\n",
    "\n",
    "# expsp1.show_in_notebook(show_table=True)\n",
    "#expsp1.as_pyplot_figure()\n",
    "\n",
    "# expsp2.show_in_notebook(show_table=True)\n",
    "#expsp2.as_pyplot_figure()\n",
    "\n",
    "# expsp3.show_in_notebook(show_table=True)\n",
    "#expsp3.as_pyplot_figure()\n",
    "\n",
    "# expsp4.show_in_notebook(show_table=True)\n",
    "#expsp4.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train.values, feature_names=x_train.columns, verbose= False, mode='regression')\n",
    "expus1 = explainer.explain_instance(np.ravel(x_test.loc[[indexusadata[0]]]), nn_reg_pred, num_features=10)\n",
    "expus2 = explainer.explain_instance(np.ravel(x_test.loc[[indexusadata[20]]]), nn_reg_pred, num_features=10)\n",
    "expus3 = explainer.explain_instance(np.ravel(x_test.loc[[indexusadata[200]]]), nn_reg_pred, num_features=10)\n",
    "expus4 = explainer.explain_instance(np.ravel(x_test.loc[[indexusadata[500]]]), nn_reg_pred, num_features=10)\n",
    "\n",
    "#expus1.show_in_notebook(show_table=True)\n",
    "#expus1.as_pyplot_figure()\n",
    "\n",
    "#expus2.show_in_notebook(show_table=True)\n",
    "#expus2.as_pyplot_figure()\n",
    "\n",
    "#expus3.show_in_notebook(show_table=True)\n",
    "#expus3.as_pyplot_figure()\n",
    "\n",
    "#expus4.show_in_notebook(show_table=True)\n",
    "#expus4.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(39931191)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.dropout = nn.Dropout(p= 0.05) # Dropout\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #lstm\n",
    "\n",
    "        self.fc_1 =  nn.Linear(hidden_size, hidden_size//2) #fully connected 1\n",
    "        self.fc = nn.Linear(hidden_size//2, 1) #fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        _, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach())) #lstm with input, hidden, and internal state\n",
    "        hn_fs = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1] #reshaping the data for Dense layer next\n",
    "        out = self.dropout(hn_fs) # Dropout\n",
    "        out = self.fc_1(out) # Dense\n",
    "        out = self.relu(out) # Activation\n",
    "        out = self.fc(out) # Dense\n",
    "        out = self.relu(out) # Activation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs =  10 #1000 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "\n",
    "input_size = 25 #number of features\n",
    "hidden_size = 175 #number of features in hidden state\n",
    "num_layers = 2 #number of stacked lstm layers\n",
    "\n",
    "x_train_tensors = Variable(torch.Tensor(x_train.values))\n",
    "x_test_tensors = Variable(torch.Tensor(x_test.values))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test.values))\n",
    "\n",
    "x_train_tensors_final = torch.reshape(x_train_tensors,   (x_train_tensors.shape[0], 1, x_train_tensors.shape[1]))\n",
    "x_test_tensors_final = torch.reshape(x_test_tensors,  (x_test_tensors.shape[0], 1, x_test_tensors.shape[1]))\n",
    "\n",
    "x_train_loader = DataLoader(x_train_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "x_test_loader = DataLoader(x_test_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "y_train_loader = DataLoader(y_train_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "y_test_loader = DataLoader(y_test_tensors, batch_size= 200, shuffle= False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING VARIABLES NO IMPORTANT: CIUDAD_CONTACTO, GENERO_CONTACTO, TIPOLOGIA, FECHA_FACTURA\n",
    "x_train_lstm = x_train.drop(columns=['CIUDAD_CONTACTO', 'GENERO_CONTACTO', 'TIPOLOGIA', 'FECHA_FACTURA'])\n",
    "x_test_lstm = x_test.drop(columns=['CIUDAD_CONTACTO', 'GENERO_CONTACTO', 'TIPOLOGIA', 'FECHA_FACTURA'])\n",
    "\n",
    "x_train_tensors = Variable(torch.Tensor(x_train_lstm.values))\n",
    "x_test_tensors = Variable(torch.Tensor(x_test_lstm.values))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test.values))\n",
    "\n",
    "x_train_tensors_final = torch.reshape(x_train_tensors,   (x_train_tensors.shape[0], 1, x_train_tensors.shape[1]))\n",
    "x_test_tensors_final = torch.reshape(x_test_tensors,  (x_test_tensors.shape[0], 1, x_test_tensors.shape[1]))\n",
    "\n",
    "x_train_loader = DataLoader(x_train_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "x_test_loader = DataLoader(x_test_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "y_train_loader = DataLoader(y_train_tensors, batch_size= 200, shuffle= False, num_workers = 2)\n",
    "y_test_loader = DataLoader(y_test_tensors, batch_size= 200, shuffle= False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM(input_size, hidden_size, num_layers) #our lstm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_val = sys.maxsize\n",
    "count_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "  for dtx, dty in zip(enumerate(x_train_loader), enumerate(y_train_loader)):\n",
    "    xtr = torch.reshape(Variable(dtx[1]), (dtx[1].shape[0], 1, dtx[1].shape[1])) # Reshape\n",
    "    outputs = lstm_model.forward(xtr) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    loss = criterion(outputs, dty[1]) # obtain the loss function\n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "  print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "  if (min_val - (min_val * 0.001)) > loss.item():\n",
    "    min_val = loss.item()\n",
    "    count_epoch = 0\n",
    "  else:\n",
    "    count_epoch = count_epoch + 1\n",
    "  if count_epoch == 10:\n",
    "    print('Model does not improve')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModelToFile(lstm_model, 'LSTMModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute it if you have the file and you don't want to execute again LSTM\n",
    "lstm_model = loadModelFromFile('LSTMModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = lstm_model(x_test_tensors_final) #forward pass\n",
    "data_predict = train_predict.data.numpy() #numpy conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, data_predict))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, data_predict))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, data_predict)))\n",
    "print('R2:', metrics.r2_score(y_test, data_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lstm'] = data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictplot(dso, dsp, dso_size, range_plot):\n",
    "    plt.axvline(x=dso_size, c='r', linestyle='--') #size of the training set\n",
    "    plt.scatter(range(0, range_plot),y_test[:range_plot], label='Actual Data') #actual plot\n",
    "    plt.scatter(range(0, range_plot), data_predict[:range_plot], label='Predicted Data') #predicted plot\n",
    "    plt.title('Time-Series Prediction')\n",
    "    plt.legend()\n",
    "    plt.savefig('../Output/timeseriespredictionLSTM.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictplot(y_test, data_predict, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "usa_iso = getISOCountry('US')\n",
    "indexusadata = getIndexFilter(testdataset, usa_iso, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexusadata_original = getIndexFilter(testdataset_original, 'US', 'NUMERO_DEUDOR_PAIS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testdataset_original.loc[[indexspaindata_original[0]]])\n",
    "f = testdataset_original.sort_values(['PRODUCTO_ID'])\n",
    "f = f.groupby(['PRODUCTO_ID', 'CONSUMER_COLOR']).IMP_VENTA_NETO_EUR.sum()\n",
    "display(f.head(20))\n",
    "data_info_original['CONSUMER_COLOR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = testdataset_original[testdataset_original.PRODUCTO_ID.str.contains('16002')].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testdataset.loc[[indexspaindata[0]]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.loc[[indexspaindata[20]]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testdataset_original.loc[[indexspaindata_original[0]]].values)\n",
    "print(testdataset_original.loc[[indexspaindata_original[1]]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(lstm_model)\n",
    "sp1 = torch.Tensor(x_test.loc[[indexspaindata[0]]].values)\n",
    "sp2 = torch.Tensor(x_test.loc[[indexspaindata[20]]].values)\n",
    "sp3 = torch.Tensor(x_test.loc[[indexspaindata[200]]].values)\n",
    "sp4 = torch.Tensor(x_test.loc[[indexspaindata[500]]].values)\n",
    "\n",
    "us1 = torch.Tensor(x_test.loc[[indexspaindata[0]]].values)\n",
    "us2 = torch.Tensor(x_test.loc[[indexspaindata[20]]].values)\n",
    "us3 = torch.Tensor(x_test.loc[[indexspaindata[200]]].values)\n",
    "us4 = torch.Tensor(x_test.loc[[indexspaindata[500]]].values)\n",
    "\n",
    "x_test_tensors_finalsp1 = torch.reshape(sp1,  (sp1.shape[0], 1, sp1.shape[1]))\n",
    "x_test_tensors_finalsp2 = torch.reshape(sp2,  (sp2.shape[0], 1, sp2.shape[1]))\n",
    "x_test_tensors_finalsp3 = torch.reshape(sp3,  (sp3.shape[0], 1, sp3.shape[1]))\n",
    "x_test_tensors_finalsp4 = torch.reshape(sp4,  (sp4.shape[0], 1, sp4.shape[1]))\n",
    "\n",
    "x_test_tensors_finalus1 = torch.reshape(us1,  (us1.shape[0], 1, us1.shape[1]))\n",
    "x_test_tensors_finalus2 = torch.reshape(us2,  (us2.shape[0], 1, us2.shape[1]))\n",
    "x_test_tensors_finalus3 = torch.reshape(us3,  (us3.shape[0], 1, us3.shape[1]))\n",
    "x_test_tensors_finalus4 = torch.reshape(us4,  (us4.shape[0], 1, us4.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensors_final_igsp1 = torch.reshape(x_test_tensors_finalsp1[0],  (1, 1, 25))\n",
    "attributionsp1 = ig.attribute(x_test_tensors_final_igsp1)\n",
    "x_test_tensors_final_igsp2 = torch.reshape(x_test_tensors_finalsp2[0],  (1, 1, 25))\n",
    "attributionsp2 = ig.attribute(x_test_tensors_final_igsp2)\n",
    "x_test_tensors_final_igsp3 = torch.reshape(x_test_tensors_finalsp3[0],  (1, 1, 25))\n",
    "attributionsp3 = ig.attribute(x_test_tensors_final_igsp3)\n",
    "x_test_tensors_final_igsp4 = torch.reshape(x_test_tensors_finalsp4[0],  (1, 1, 25))\n",
    "attributionsp4 = ig.attribute(x_test_tensors_final_igsp4)\n",
    "\n",
    "x_test_tensors_final_igus1 = torch.reshape(x_test_tensors_finalus1[0],  (1, 1, 25))\n",
    "attributionus1 = ig.attribute(x_test_tensors_final_igus1)\n",
    "x_test_tensors_final_igus2 = torch.reshape(x_test_tensors_finalus2[0],  (1, 1, 25))\n",
    "attributionus2 = ig.attribute(x_test_tensors_final_igus2)\n",
    "x_test_tensors_final_igus3 = torch.reshape(x_test_tensors_finalus3[0],  (1, 1, 25))\n",
    "attributionus3 = ig.attribute(x_test_tensors_final_igus3)\n",
    "x_test_tensors_final_igus4 = torch.reshape(x_test_tensors_finalus4[0],  (1, 1, 25))\n",
    "attributionus4 = ig.attribute(x_test_tensors_final_igus4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importance(title, features, importance, xtitle):\n",
    "    plt.barh(range(0, features.size), importance, align = 'center')\n",
    "    plt.yticks(range(0, features.size), features, wrap = True, fontsize = 7)\n",
    "    plt.ylabel(xtitle)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importance('Variable Importance', x_test.columns, attributionsp1.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionsp2.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionsp3.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionsp4.detach().numpy()[0][0], 'Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importance('Variable Importance', x_test.columns, attributionus1.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionus2.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionus3.detach().numpy()[0][0], 'Feature')\n",
    "visualize_importance('Variable Importance', x_test.columns, attributionus4.detach().numpy()[0][0], 'Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importances_LC(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances[:6], align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True, fontsize = 10)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = LayerConductance(lstm_model, lstm_model.fc_1)\n",
    "layer_1_valssp1 = layer_1.attribute(x_test_tensors_final_igsp1)\n",
    "layer_1_valssp1 = (layer_1_valssp1.detach().numpy())\n",
    "layer_1_valssp2 = layer_1.attribute(x_test_tensors_final_igsp2)\n",
    "layer_1_valssp2 = (layer_1_valssp2.detach().numpy())\n",
    "layer_1_valssp3 = layer_1.attribute(x_test_tensors_final_igsp3)\n",
    "layer_1_valssp3 = (layer_1_valssp3.detach().numpy())\n",
    "layer_1_valssp4 = layer_1.attribute(x_test_tensors_final_igsp4)\n",
    "layer_1_valssp4 = (layer_1_valssp4.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = LayerConductance(lstm_model, lstm_model.fc_1)\n",
    "layer_1_valsus1 = layer_1.attribute(x_test_tensors_final_igus1)\n",
    "layer_1_valsus1 = (layer_1_valsus1.detach().numpy())\n",
    "layer_1_valsus2 = layer_1.attribute(x_test_tensors_final_igus2)\n",
    "layer_1_valsus2 = (layer_1_valsus2.detach().numpy())\n",
    "layer_1_valsus3 = layer_1.attribute(x_test_tensors_final_igus3)\n",
    "layer_1_valsus3 = (layer_1_valsus3.detach().numpy())\n",
    "layer_1_valsus4 = layer_1.attribute(x_test_tensors_final_igus4)\n",
    "layer_1_valsus4 = (layer_1_valsus4.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances_LC(range(6), np.mean(layer_1_valssp1, axis = 0), title=\"Average Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valssp2, axis = 0), title=\"Cumulative Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valssp3, axis = 0), title=\"Average Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valssp4, axis = 0), title=\"Average Neuron Importances\", axis_title = \"Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances_LC(range(6), np.mean(layer_1_valsus1, axis = 0), title=\"Average Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valsus2, axis = 0), title=\"Cumulative Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valsus3, axis = 0), title=\"Average Neuron Importances\", axis_title= \"Neurons\")\n",
    "visualize_importances_LC(range(6), np.mean(layer_1_valsus4, axis = 0), title=\"Average Neuron Importances\", axis_title = \"Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importances_NC(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.barh(x_pos, importances, align='center')\n",
    "        plt.yticks(x_pos, feature_names, wrap=True, fontsize = 7)\n",
    "        plt.ylabel(axis_title)\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_cond = NeuronConductance(lstm_model, lstm_model.fc_1)\n",
    "neuron_cond_vals_5sp1 = neuron_cond.attribute(x_test_tensors_final_igsp1, neuron_selector=5)\n",
    "neuron_cond_vals_5sp2 = neuron_cond.attribute(x_test_tensors_final_igsp2, neuron_selector=5)\n",
    "neuron_cond_vals_5sp3 = neuron_cond.attribute(x_test_tensors_final_igsp3, neuron_selector=5)\n",
    "neuron_cond_vals_5sp4 = neuron_cond.attribute(x_test_tensors_final_igsp4, neuron_selector=5)\n",
    "neuron_cond_vals_5us1 = neuron_cond.attribute(x_test_tensors_final_igus1, neuron_selector=5)\n",
    "neuron_cond_vals_5us2 = neuron_cond.attribute(x_test_tensors_final_igus2, neuron_selector=5)\n",
    "neuron_cond_vals_5us3 = neuron_cond.attribute(x_test_tensors_final_igus3, neuron_selector=5)\n",
    "neuron_cond_vals_5us4 = neuron_cond.attribute(x_test_tensors_final_igus4, neuron_selector=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5sp1.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5sp2.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5sp3.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5sp4.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5us1.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5us2.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5us3.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")\n",
    "visualize_importances_NC(x_train.columns, neuron_cond_vals_5us4.mean(dim=0).detach().numpy()[0], title=\"Average Feature Importances for Neuron 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committee of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXtrain = results[['xgboost', 'lstm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = xgb.XGBRegressor(learning_rate =0.01, n_estimators=320, max_depth=20, min_child_weight=0.8, subsample=1, nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner.fit(newXtrain, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = meta_learner.predict(newXtrain)\n",
    "results['CommExps'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mean_absolute_percentage_error(results['Import'], results['CommExps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the base model is learnt from the training data. Then, a second model (the error model) is trained on a validation set to predict the squared difference between the predictions and the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeVal(dt, ind, drange):\n",
    "    money = []\n",
    "    dayslist = []\n",
    "    counter = []\n",
    "    day = drange\n",
    "    i = 0\n",
    "    dayslist.append(str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][2]) + '/' + str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][1]) + '/' + str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][0]))\n",
    "    while True:\n",
    "        dayformatted = str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][2]) + '/' + str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][1]) + '/' + str(dt.loc[[ind[(len(ind) - 1) - i]]].values[0][0])\n",
    "        if dayformatted not in dayslist:\n",
    "            dayslist.append(dayformatted)\n",
    "            day = day - 1\n",
    "            counter.append(i)\n",
    "        if day == 0:\n",
    "            dayslist = dayslist[:drange]\n",
    "            break\n",
    "        i = i + 1\n",
    "    return dayslist, counter\n",
    "def sumPredictions(predictions, indexes):\n",
    "    sum = 0\n",
    "    sumpred = []\n",
    "    aux = 0\n",
    "    for p in predictions.values:\n",
    "        sum = sum + float(p)\n",
    "        if aux in indexes:\n",
    "            sumpred.append(round(sum, 2))\n",
    "            sum = 0\n",
    "        aux = aux + 1\n",
    "    return sumpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexspaindata = getIndexFilter(testdataset, 14, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexspaindata_original = getIndexFilter(testdataset_original, 'ES', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "usa_iso = getISOCountry('US')\n",
    "indexusadata = getIndexFilter(testdataset, usa_iso, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexusadata_original = getIndexFilter(testdataset_original, 'US', 'NUMERO_DEUDOR_PAIS_ID')\n",
    "ita_iso = getISOCountry('IT')\n",
    "indexitadata = getIndexFilter(testdataset, ita_iso, 'NUMERO_DEUDOR_PAIS_ID')\n",
    "indexitadata_original = getIndexFilter(testdataset_original, 'IT', 'NUMERO_DEUDOR_PAIS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SpaintakeVal, indexcountersp = takeVal(testdataset_original, indexspaindata_original, 40)\n",
    "UsatakeVal, indexcounterus = takeVal(testdataset_original, indexusadata_original, 40)\n",
    "ItatakeVal, indexcounterit = takeVal(testdataset_original, indexitadata_original, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "impsp = sumPredictions(results[results.index.isin(indexspaindata)]['Import'], indexcountersp)\n",
    "impus = sumPredictions(results[results.index.isin(indexusadata)]['Import'], indexcounterus)\n",
    "impita = sumPredictions(results[results.index.isin(indexitadata)]['Import'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp = sumPredictions(results[results.index.isin(indexspaindata)]['lm'], indexcountersp)\n",
    "predus = sumPredictions(results[results.index.isin(indexusadata)]['lm'], indexcounterus)\n",
    "predita = sumPredictions(results[results.index.isin(indexitadata)]['lm'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lm_sp = round(metrics.mean_squared_error(impsp, predsp) ** 0.5, 2)\n",
    "upper = impsp + st_dev_lm_sp\n",
    "lower = impsp - st_dev_lm_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lm_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color = 'k')\n",
    "plt.plot(SpaintakeVal, predsp, '.', color = 'g')\n",
    "plt.fill_between(SpaintakeVal, upper, lower, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lm_us = round(metrics.mean_squared_error(impus, predus) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color =  'k')\n",
    "plt.plot(UsatakeVal, predus, '.', color =  'g')\n",
    "plt.fill_between(UsatakeVal, (impus + st_dev_lm_us), (impus - st_dev_lm_us), alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lm_ita = round(metrics.mean_squared_error(impita, predita) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color =  'k')\n",
    "plt.plot(ItatakeVal, predita, '.', color =  'g')\n",
    "plt.fill_between(ItatakeVal, (impita + st_dev_lm_ita), (impita - st_dev_lm_ita), alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spain: ', st_dev_lm_sp)\n",
    "print('US: ', st_dev_lm_us)\n",
    "print('Italy: ', st_dev_lm_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp_xgboost = sumPredictions(results[results.index.isin(indexspaindata)]['xgboost'], indexcountersp)\n",
    "predus_xgboost = sumPredictions(results[results.index.isin(indexusadata)]['xgboost'], indexcounterus)\n",
    "predita_xgboost = sumPredictions(results[results.index.isin(indexitadata)]['xgboost'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_xgboost_sp = round(metrics.mean_squared_error(impsp, predsp_xgboost) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color =  'k')\n",
    "plt.plot(SpaintakeVal, predsp_xgboost, '.', color =  'g')\n",
    "plt.fill_between(range(len(SpaintakeVal)), impsp + st_dev_xgboost_sp, impsp - st_dev_xgboost_sp, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_xgboost_us = round(metrics.mean_squared_error(impus, predus_xgboost) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color = 'k')\n",
    "plt.plot(UsatakeVal, predus_xgboost, '.', color = 'g')\n",
    "plt.fill_between(range(len(UsatakeVal)), (impus + st_dev_xgboost_us), (impus - st_dev_xgboost_us), alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_xgboost_ita = round(metrics.mean_squared_error(impita, predita_xgboost) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color = 'k')\n",
    "plt.plot(ItatakeVal, predita_xgboost, '.', color = 'g')\n",
    "plt.fill_between(range(len(ItatakeVal)), (impita + st_dev_xgboost_ita), (impita - st_dev_xgboost_ita), alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spain: ', st_dev_xgboost_sp)\n",
    "print('US: ', st_dev_xgboost_us)\n",
    "print('Italy: ', st_dev_xgboost_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp_nn = sumPredictions(results[results.index.isin(indexspaindata)]['nn'], indexcountersp)\n",
    "predus_nn = sumPredictions(results[results.index.isin(indexusadata)]['nn'], indexcounterus)\n",
    "predita_nn = sumPredictions(results[results.index.isin(indexitadata)]['nn'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_nn_sp = round(metrics.mean_squared_error(impsp, predsp_nn) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color =  'k')\n",
    "plt.plot(SpaintakeVal, predsp_nn, '.', color =  'g')\n",
    "plt.fill_between(SpaintakeVal, impsp + st_dev_nn_sp, impsp - st_dev_nn_sp, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_nn_us = round(metrics.mean_squared_error(impus, predus_nn) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color = 'k')\n",
    "plt.plot(UsatakeVal, predus_nn, '.', color = 'g')\n",
    "plt.fill_between(UsatakeVal, (impus + st_dev_nn_us), (impus - st_dev_nn_us), alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_nn_ita = round(metrics.mean_squared_error(impita, predita_nn) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color = 'k')\n",
    "plt.plot(ItatakeVal, predita_nn, '.', color = 'g')\n",
    "plt.fill_between(ItatakeVal, (impita + st_dev_nn_ita), (impita - st_dev_nn_ita), alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spain: ', st_dev_nn_sp)\n",
    "print('US: ', st_dev_nn_us)\n",
    "print('Italy: ', st_dev_nn_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp_lstm = sumPredictions(results[results.index.isin(indexspaindata)]['lstm'], indexcountersp)\n",
    "predus_lstm = sumPredictions(results[results.index.isin(indexusadata)]['lstm'], indexcounterus)\n",
    "predita_lstm = sumPredictions(results[results.index.isin(indexitadata)]['lstm'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lstm_sp = round(metrics.mean_squared_error(impsp, predsp_lstm) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color =  'k')\n",
    "plt.plot(SpaintakeVal, predsp_lstm, '.', color =  'g')\n",
    "plt.fill_between(SpaintakeVal, impsp + st_dev_lstm_sp, impsp - st_dev_lstm_sp, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lstm_us = round(metrics.mean_squared_error(impus, predus_lstm) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color = 'k')\n",
    "plt.plot(UsatakeVal, predus_lstm, '.', color = 'g')\n",
    "plt.fill_between(UsatakeVal, (impus + st_dev_lstm_us), (impus - st_dev_lstm_us), alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_lstm_ita = round(metrics.mean_squared_error(impita, predita_lstm) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color = 'k')\n",
    "plt.plot(ItatakeVal, predita_lstm, '.', color = 'g')\n",
    "plt.fill_between(ItatakeVal, (impita + st_dev_lstm_ita), (impita - st_dev_lstm_ita), alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spain: ', st_dev_lstm_sp)\n",
    "print('US: ', st_dev_lstm_us)\n",
    "print('Italy: ', st_dev_lstm_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsp_CommExps = sumPredictions(results[results.index.isin(indexspaindata)]['CommExps'], indexcountersp)\n",
    "predus_CommExps = sumPredictions(results[results.index.isin(indexusadata)]['CommExps'], indexcounterus)\n",
    "predita_CommExps = sumPredictions(results[results.index.isin(indexitadata)]['CommExps'], indexcounterit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_CommExps_sp = round(metrics.mean_squared_error(impsp, predsp_CommExps) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color =  'k')\n",
    "plt.plot(SpaintakeVal, predsp_CommExps, '.', color =  'g')\n",
    "plt.fill_between(SpaintakeVal, impsp + st_dev_CommExps_sp, impsp - st_dev_CommExps_sp, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_CommExps_us = round(metrics.mean_squared_error(impus, predus_CommExps) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color = 'k')\n",
    "plt.plot(UsatakeVal, predus_CommExps, '.', color = 'g')\n",
    "plt.fill_between(UsatakeVal, (impus + st_dev_CommExps_us), (impus - st_dev_CommExps_us), alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dev_CommExps_ita = round(metrics.mean_squared_error(impita, predita_CommExps) ** 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color = 'k')\n",
    "plt.plot(ItatakeVal, predita_CommExps, '.', color = 'g')\n",
    "plt.fill_between(ItatakeVal, (impita + st_dev_CommExps_ita), (impita - st_dev_CommExps_ita), alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'Prediction', 'Prediction Interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spain: ', st_dev_CommExps_sp)\n",
    "print('US: ', st_dev_CommExps_us)\n",
    "print('Italy: ', st_dev_CommExps_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SpaintakeVal, impsp, '.', color =  'k')\n",
    "plt.plot(SpaintakeVal, predsp, '.', color = 'r')\n",
    "plt.plot(SpaintakeVal, predsp_xgboost, '.', color =  'b')\n",
    "plt.plot(SpaintakeVal, predsp_nn, '.', color =  'y')\n",
    "plt.plot(SpaintakeVal, predsp_lstm, '.', color =  'c')\n",
    "plt.plot(SpaintakeVal, predsp_CommExps, '.', color =  'g')\n",
    "plt.fill_between(SpaintakeVal, impsp + st_dev_CommExps_sp, impsp - st_dev_CommExps_sp, alpha=0.2)\n",
    "plt.xticks(SpaintakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Spain')\n",
    "plt.legend(['Real Import', 'MLR', 'XGBoost', 'MLP Regressor', 'LSTM', 'Hybrid approach'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UsatakeVal, impus, '.', color =  'k')\n",
    "plt.plot(UsatakeVal, predus, '.', color = 'r')\n",
    "plt.plot(UsatakeVal, predus_xgboost, '.', color =  'b')\n",
    "plt.plot(UsatakeVal, predus_nn, '.', color =  'y')\n",
    "plt.plot(UsatakeVal, predus_lstm, '.', color =  'c')\n",
    "plt.plot(UsatakeVal, predus_CommExps, '.', color =  'g')\n",
    "plt.fill_between(UsatakeVal, impus + st_dev_CommExps_us, impus - st_dev_CommExps_us, alpha=0.2)\n",
    "plt.xticks(UsatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - US')\n",
    "plt.legend(['Real Import', 'MLR', 'XGBoost', 'MLP Regressor', 'LSTM', 'Hybrid approach'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ItatakeVal, impita, '.', color =  'k')\n",
    "plt.plot(ItatakeVal, predita, '.', color = 'r')\n",
    "plt.plot(ItatakeVal, predita_xgboost, '.', color =  'b')\n",
    "plt.plot(ItatakeVal, predita_nn, '.', color =  'y')\n",
    "plt.plot(ItatakeVal, predita_lstm, '.', color =  'c')\n",
    "plt.plot(ItatakeVal, predita_CommExps, '.', color =  'g')\n",
    "plt.fill_between(ItatakeVal, impita + st_dev_CommExps_ita, impita - st_dev_CommExps_ita, alpha=0.2)\n",
    "plt.xticks(ItatakeVal, rotation=85)\n",
    "plt.title('Prediction Interval (€) - Italy')\n",
    "plt.legend(['Real Import', 'MLR', 'XGBoost', 'MLP Regressor', 'LSTM', 'Hybrid approach'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(st_dev_CommExps_sp)\n",
    "display(st_dev_CommExps_us)\n",
    "display(st_dev_CommExps_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((impita + st_dev_CommExps_ita))\n",
    "display((impita - st_dev_CommExps_ita))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "for p, r in zip(predsp_CommExps, impsp):\n",
    "    if ((r + st_dev_CommExps_sp) < p):\n",
    "        print('Deviance: ', round((1-((r + st_dev_CommExps_sp) / p))))\n",
    "    if ((r - st_dev_CommExps_sp) > p):\n",
    "        print('Deviance: ', round((1 - (p / (r-st_dev_CommExps_sp))) * 100, 2))\n",
    "        mean.append((round((1 - (p / (r-st_dev_CommExps_sp))) * 100, 2)))\n",
    "print('Mean error: ', round(sum(mean)/len(mean), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "for p, r in zip(predus_CommExps, impus):\n",
    "    if ((r + st_dev_CommExps_us) < p):\n",
    "        print(p)\n",
    "    if ((r - st_dev_CommExps_us) > p):\n",
    "        print('Deviance: ', round((1 - (p / (r-st_dev_CommExps_us))) * 100, 2))\n",
    "        mean.append(round((1 - (p / (r-st_dev_CommExps_us))) * 100, 2))\n",
    "print('Mean error: ', round(sum(mean)/len(mean), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "for p, r in zip(predita_CommExps, impita):\n",
    "    if ((r + st_dev_CommExps_ita) < p):\n",
    "        print(p)\n",
    "    if ((r - st_dev_CommExps_ita) > p):\n",
    "        print('Deviance: ', round((1 - (p / (r-st_dev_CommExps_ita))) * 100, 2))\n",
    "        mean.append(round((1 - (p / (r-st_dev_CommExps_ita))) * 100, 2))\n",
    "print('Mean error: ', round(sum(mean)/len(mean), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['CommExps'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "shapiro_test = stats.shapiro(results['CommExps'].values)\n",
    "print('Statistics=%.3f, p=%.3f' % (shapiro_test.statistic, shapiro_test.pvalue))\n",
    "if shapiro_test.pvalue > 0.05:\n",
    "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from light_famd import FAMD\n",
    "from sklearn import preprocessing\n",
    "import category_encoders as ce\n",
    "from bioinfokit.visuz import cluster\n",
    "import threading\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaldata = pd.read_csv('../Data/Venta_Consumidor_Producto_UPC_PrevVTA_2018-2020.csv')\n",
    "originaldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "dataset = originaldata.drop(columns=['ANO_MES_FACTURA', 'CANAL_VENTA_DESC', 'TEMPORADA_COMERCIAL_DESC', 'MATERIAL_ID', 'ESFUERZO_VENTA_DESC', 'NUMERO_DEUDOR', 'NUMERO_DEUDOR_PAIS_DESC', 'CONCEPTO', 'LINEA',  'PAIS_CONTACTO', 'PAIS_CONTACTO_DESC'])\n",
    "# Columns I have\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "First of all, I am going to see if there are missing data. In the file called Estructura Datos, it says that I used CANAL_VENTA_ID  = 20 because it will be the only one, that's not true, because there are more IDs, by the moment I only use 20 but it would be nice to know which one I should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_missing_data = dataset[dataset.CANAL_VENTA_ID == 20]\n",
    "md = pd.DataFrame(dataset_missing_data.isnull().sum(), columns=['Number'])\n",
    "md['Percentage'] = dataset_missing_data.isnull().sum()/originaldata.shape[0]*100\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUSTOMER_ID: I am not interested in it, unless I could create a profile.\n",
    "- IMP_VENTA_NETO_EUR: Ghost items sold, by the moment I am going to remove these rows, because they are not adding any kind of information and I cannot add NV because I will have problems when I will calculate all the stuff.\n",
    "- TALLA:\n",
    "    - I can remove them\n",
    "    - But I think it would be better to add NV (no informado) as it is used in other columns for the same purpose\n",
    "- ESFUERZO_VENTA_ID: I don't know what is that\n",
    "- GENERO_PRODUCTO: the basics are Kids, Women and Men. I can treat nulls as NV (no informado) or consider it as unisex. I prefer NV but I think it is important to keep and don't remove them, they can add value information although there are nulls\n",
    "- CATEGORIA: I don't think this is an important column, I think I will not use it, by the moment I let them be null\n",
    "- TIPOLOGIA: I don't think either it is an important column, as before I let them be null\n",
    "- CONSUMER_COLOR: Same as before, I can use COLOR instead of this\n",
    "- GENERO_CONTACTO: I am not sure what this is, I can put NV by the moment\n",
    "- PAIS_CONTACTO: It can be interested to see what products are the most selled in each country and compare them. If one of the others columns refered to the customer are filled, I can extract the country, if not, the best thing I can do is to put NV\n",
    "- CIUDAD_CONTACTO: Same as before, in this case, for me is not that relevant\n",
    "- IDIOMA_CONTACTO: Same as before\n",
    "\n",
    "There aren't NA/nan/Na strings and NV can only be found in the specified columns in the file I said before.\n",
    "\n",
    "Firstly, I am going to select only the data with CANAL_VENTA_ID equal to 20. Secondly, I am going to remove all the rows where IMP_VENTA_NETO_EUR is equal to null and after I am going to replace all nulls with NV (no informado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[(dataset.CANAL_VENTA_ID == 20)]\n",
    "dataset = dataset[(dataset.IMP_VENTA_NETO_EUR.isnull() == False) | (dataset.IMP_VENTA_NETO_EUR == 0)]\n",
    "dataset.fillna(\"NV\", inplace = True)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only consider the products from Camper. JERARQUIA_PROD_ID = '1*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for k, v in zip(dataset.index, dataset.JERARQUIA_PROD_ID):\n",
    "    if v[0] == '1':\n",
    "        indexes.append(k)\n",
    "dataset = dataset[dataset.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers could be find in the following columns (numerical, not definit):\n",
    "- IMP_VENTA_NETO_EUR\n",
    "- EDAD_COMPRA\n",
    "\n",
    "Firstly, I am going to plot the infomation in quartiles and check if I can visualize possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierdata = dataset[dataset.FACTURA_CLASE_DOCUMENTO_ID == 'ZTON']\n",
    "# Univariate outlier detection\n",
    "plt.boxplot(outlierdata['IMP_VENTA_NETO_EUR'])\n",
    "plt.title('BoxPlot - Net sales amount')\n",
    "plt.savefig('../Output/BoxPlotNetsalesamount.png')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(outlierdata['EDAD_COMPRA'])\n",
    "plt.title('BoxPlot - Age purchase')\n",
    "plt.savefig('../Output/BoxPlotAgepurchase.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, I have found several outliers. For instance, it is not possible to sell a product with a price of 0€, or that age of the customer is less than 18 years old. Moreover, I have a lot of high prices so let's analyse all these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "highprices = outlierdata[outlierdata.IMP_VENTA_NETO_EUR >=400][['IMP_VENTA_NETO_EUR', 'PRODUCTO_ID', 'CATEGORIA', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GRUPO_ARTICULO']].sort_values(by = 'IMP_VENTA_NETO_EUR',ascending=False).head(10)\n",
    "lowprices = outlierdata[['IMP_VENTA_NETO_EUR', 'PRODUCTO_ID', 'CATEGORIA', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GRUPO_ARTICULO']].sort_values(by = 'IMP_VENTA_NETO_EUR',ascending=True).head(10)\n",
    "display(highprices)\n",
    "display(lowprices)\n",
    "print('Number of products cheaper than 10€: '+str(outlierdata[outlierdata.IMP_VENTA_NETO_EUR < 10].count().IMP_VENTA_NETO_EUR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above, I have found some products that are very expensive, as my main aim of the project is to predict the shoes sold, I can remove the product which don't have the category of shoes.\n",
    "\n",
    "Category Summarize:\n",
    "- 01 -> Zapatos Adulto\n",
    "- 02 -> Ropa\n",
    "- 03 -> Zapatos Factorys (producto que se fabrica expresamente para Outlets)\n",
    "- 04 -> Bolsos\n",
    "- 05 -> Complementos (Accesorios)\n",
    "- 06 -> Complementos VOR (Accesorios Volvo Ocean Race)\n",
    "- 07 -> PLV\n",
    "- 08 -> Kids\n",
    "- 10 -> First Walkers\n",
    "\n",
    "I am not interested in 2, 5, 6 and 7. So I am going to remove them from my dataset. Moreover, I will remove the values which have IMP_VENTA_NETO_EUR less than 10€, such that they are errors (not possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierdata = outlierdata[(~outlierdata.GRUPO_ARTICULO_PRODUCTO_ID.isin([2, 5, 6, 7])) & (outlierdata.IMP_VENTA_NETO_EUR >= 10)]\n",
    "plt.boxplot(outlierdata['IMP_VENTA_NETO_EUR'])\n",
    "plt.show()\n",
    "display(outlierdata[outlierdata.IMP_VENTA_NETO_EUR >=400][['IMP_VENTA_NETO_EUR', 'PRODUCTO_ID', 'CATEGORIA', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GRUPO_ARTICULO']].sort_values(by = 'IMP_VENTA_NETO_EUR',ascending=False).head(10))\n",
    "display(outlierdata[['IMP_VENTA_NETO_EUR', 'PRODUCTO_ID', 'CATEGORIA', 'NUMERO_DEUDOR_PAIS_ID', 'JERARQUIA_PROD_ID', 'GRUPO_ARTICULO_PRODUCTO_ID', 'GRUPO_ARTICULO']].sort_values(by = 'IMP_VENTA_NETO_EUR',ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all the possible outliers have been removed, I cannot consider the remaining values as outliers such that all of them are possible, (responsable camper said that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to analyse all the outliers in the field called Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noageinfo = outlierdata[outlierdata.EDAD_SN == 'N']['EDAD_SN'].count()\n",
    "ageinfo = outlierdata[outlierdata.EDAD_SN == 'S']['EDAD_SN'].count()\n",
    "print('Number of rows without informed age:', noageinfo)\n",
    "print('number of rows with information of age ', ageinfo)\n",
    "print('Representation of no informed age: ', round(noageinfo/ageinfo*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I discover that approximately my 39% of the data don't have informed the age, so the outliers I found previously, in fact they are non informed values. As this is a huge amount of information, I cannot remove them so I will consider them as a non informed, NV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = outlierdata.drop(columns = ['GRUPO_ARTICULO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data featuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, I am going to extract some features from the dataset that it will help me to understand and create the future models. Also, my main target is to remove data redundancy and prepare it for the analysis.\n",
    "First, I am not interested in having the full date saved because I alreade have month and year in a different columns. So, I will save only the day of the sale. Moreover, In some columns I have information in english, spanish, ... Which the meaning is the same, so basically I am going to unify them into a single category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.FECHA_FACTURA =  dataset['FECHA_FACTURA'].str.split('-', expand = True)[2]\n",
    "dataset['CREMALLERA'].replace(to_replace= 'YES', value = 'SI', inplace= True)\n",
    "dataset['CREMALLERA'].replace(to_replace= 'YES', value = 'SI', inplace= True)\n",
    "dataset['CORDONES'].replace(to_replace= 'With laces', value = 'Con cordones', inplace= True)\n",
    "dataset['CORDONES'].replace(to_replace= 'Without laces', value = 'Sin cordones', inplace= True)\n",
    "dataset['OUTSOLE_SUELA_SUBTIPO'].replace(to_replace= 'MEDIUM ( + 4.5 cm to 6 cm)', value = 'MEDIUM', inplace= True)\n",
    "dataset['OUTSOLE_SUELA_SUBTIPO'].replace(to_replace= 'HIGH (+ 6 cm)', value = 'HIGH', inplace= True)\n",
    "dataset['PLANTILLA_EXTRAIBLE'].replace(to_replace= 'No extractable', value = 'No extraible', inplace= True)\n",
    "dataset['PLANTILLA_EXTRAIBLE'].replace(to_replace= 'Extractable', value = 'Extraible', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights\n",
    "\n",
    "Now, I am going to analyse the profit by year. As I said before, I use CANAL_VENTA_ID = 20 (Online). Total year (2018) profit is more or less 24 milions of €/$ (?). So now, I will analyse in which months they earn more money. After, I will see what is the percentage got in each month from the total earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_year_sold_2018 = round(dataset[dataset.ANO_FACTURA == 2018].IMP_VENTA_NETO_EUR.sum(),2)\n",
    "total_year_sold_2019 = round(dataset[dataset.ANO_FACTURA == 2019].IMP_VENTA_NETO_EUR.sum(),2)\n",
    "total_year_sold_2020 = round(dataset[dataset.ANO_FACTURA == 2020].IMP_VENTA_NETO_EUR.sum(),2)\n",
    "print('Total year profit 2018: '+str(total_year_sold_2018)+'€')\n",
    "print('Total year profit 2019: '+str(total_year_sold_2019)+'€')\n",
    "print('Total year profit 2020: '+str(total_year_sold_2020)+'€')\n",
    "\n",
    "total_month_sold_2018 = round(dataset[dataset.ANO_FACTURA == 2018].groupby(['MES_FACTURA']).IMP_VENTA_NETO_EUR.sum(),2)\n",
    "total_month_sold_2018 = np.array(total_month_sold_2018)\n",
    "total_month_sold_2019 = round(dataset[dataset.ANO_FACTURA == 2019].groupby(['MES_FACTURA']).IMP_VENTA_NETO_EUR.sum(),2)\n",
    "total_month_sold_2019 = np.array(total_month_sold_2019)\n",
    "total_month_sold_2020 = round(dataset[dataset.ANO_FACTURA == 2020].groupby(['MES_FACTURA']).IMP_VENTA_NETO_EUR.sum(),2)\n",
    "total_month_sold_2020 = np.array(total_month_sold_2020)\n",
    "name_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "plt.bar(name_months, total_month_sold_2018)\n",
    "plt.title('Sold in 2018')\n",
    "plt.savefig('../Output/sold2018.png')\n",
    "plt.show()\n",
    "sns.kdeplot(total_month_sold_2018, bw=0.3) # 0.2\n",
    "plt.title('Density (2018)')\n",
    "plt.savefig('../Output/densitysold2018.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(name_months, total_month_sold_2019)\n",
    "plt.title('Sold in 2019')\n",
    "plt.savefig('../Output/sold2019.png')\n",
    "plt.show()\n",
    "sns.kdeplot(total_month_sold_2019, bw=0.3) # 0.2\n",
    "plt.title('Density (2019)')\n",
    "plt.savefig('../Output/densitysold2019.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(name_months, total_month_sold_2020)\n",
    "plt.title('Sold in 2020')\n",
    "plt.savefig('../Output/sold2020.png')\n",
    "plt.show()\n",
    "sns.kdeplot(total_month_sold_2020, bw=0.3) # 0.2\n",
    "plt.title('Density (2020)')\n",
    "plt.savefig('../Output/densitysold2020.png')\n",
    "plt.show()\n",
    "\n",
    "print('Benefit from each month(%):\\n')\n",
    "print('\\t 2018 \\t 2019 \\t 2020')\n",
    "for i in range(0, 12):\n",
    "    print(name_months[i]+': \\t '+str(round(total_month_sold_2018[i]/(total_year_sold_2018)*100,2))+' \\t '+\n",
    "    str(round(total_month_sold_2019[i]/(total_year_sold_2019)*100,2))+' \\t '+str(round(total_month_sold_2020[i]/(total_year_sold_2020)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, it can be seen in the first graph, the months where the profit is higher are January, June and July. Probably this is because the discounts they do (Sales), we will see it later.\n",
    "\n",
    "In 2019, the highest profit is in January, July and December. Notice that in average all the sales have been increased every month over the previous year.\n",
    "\n",
    "In 2020, we have a strange phenomenon, when I told before that the sales were increasing in comparison between 2018 and 2019 month by month, in this case the sales have decreased in all months. The reason is simple, COVID-19. One of the measures to stop COVID-19 was to stay home and don't go outside. Although Camper has online sevices, if people cannot go outside, they are not going to use their shoes so the outsole doesn't wear out and people don't need to buy them. This is the reason for the decreasing sales. Also, it can be seen that in July and Novembre we have the maximum profit of the year. In the case of July makes sense that we got the max. profit but why in November? As before, the reason is simple, almost all countries advanced the sales period for letting people to go outside to spend money in a desperate way for letting the commerce increase their income.\n",
    "\n",
    "As it can be seen in the distributions, they look like a bimodal representation, where it reveals that there are two different types, when it is sales and when it is not. However there are only 3 months of sales, it conditions the density plot because they have a lot of weight (high volume)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to extract are the bests selling products for each year.\n",
    "The top ten products are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales_year_2018 = round(dataset[dataset.ANO_FACTURA == 2018].groupby(['PRODUCTO_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "product_sales_year_2019 = round(dataset[dataset.ANO_FACTURA == 2019].groupby(['PRODUCTO_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "product_sales_year_2020 = round(dataset[dataset.ANO_FACTURA == 2020].groupby(['PRODUCTO_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "product_sales_year_2018 = product_sales_year_2018.sort_values(ascending = False).head(10)\n",
    "product_sales_year_2019 = product_sales_year_2019.sort_values(ascending = False).head(10)\n",
    "product_sales_year_2020 = product_sales_year_2020.sort_values(ascending = False).head(10)\n",
    "print('Most sold products 2018:')\n",
    "display(product_sales_year_2018)\n",
    "print('\\nMost sold products 2019:')\n",
    "display(product_sales_year_2019)\n",
    "print('\\nMost sold products 2020:')\n",
    "display(product_sales_year_2020)\n",
    "\n",
    "print('\\nThese are the products common products sold each year:\\n')\n",
    "for k1, k2 in zip(product_sales_year_2018.index, product_sales_year_2019.index):\n",
    "    if k1 in product_sales_year_2019.index or k1 in product_sales_year_2020.index:\n",
    "        print(k1)\n",
    "    if k2 in product_sales_year_2020.index:\n",
    "        print(k2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it is interesting to see which are the bests selling products each month, in this way, we will be able to see if there's any month where the product hits a maximum. I will be focused on the top ten products. It is possible to do this for each product, in this way I would discover if there are any product that during the year has lower sells but when it is in sales, it has a boom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales_month = dataset.groupby(['MES_FACTURA','PRODUCTO_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "product_sales_month = pd.DataFrame(product_sales_month)\n",
    "months = []\n",
    "product_id = []\n",
    "money = []\n",
    "\n",
    "for k, v in zip(product_sales_month.index, product_sales_month.IMP_VENTA_NETO_EUR):\n",
    "    months.append(k[0])\n",
    "    product_id.append(k[1])\n",
    "    money.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_month_sales = pd.DataFrame({\n",
    "    'Month': months,\n",
    "    'Product': product_id,\n",
    "    'Money': money\n",
    "})\n",
    "print('Top products best month sales 2018:\\n')\n",
    "for i in range(0, len(product_sales_year_2018)):\n",
    "    val_money = round(product_month_sales[(product_month_sales.Product == product_sales_year_2018.index[i])].Money.max(), 2)\n",
    "    print(product_month_sales[(product_month_sales.Product == product_sales_year_2018.index[i]) & (product_month_sales.Money.astype(int) == val_money.astype(int))])\n",
    "\n",
    "print('\\nTop products best month sales 2019:\\n')\n",
    "for i in range(0, len(product_sales_year_2019)):\n",
    "    val_money = round(product_month_sales[(product_month_sales.Product == product_sales_year_2019.index[i])].Money.max(), 2)\n",
    "    print(product_month_sales[(product_month_sales.Product == product_sales_year_2019.index[i]) & (product_month_sales.Money.astype(int) == val_money.astype(int))])  \n",
    "\n",
    "print('\\nTop products best month sales 2020:\\n')\n",
    "for i in range(0, len(product_sales_year_2020)):\n",
    "    val_money = round(product_month_sales[(product_month_sales.Product == product_sales_year_2020.index[i])].Money.max(), 2)\n",
    "    print(product_month_sales[(product_month_sales.Product == product_sales_year_2020.index[i]) & (product_month_sales.Money.astype(int) == val_money.astype(int))])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, in 2018, 8/10 products are top sellers in Jan/June/July (Sales) and finally, in 2019 and  in 2020 7/10 products are top sold in sales.\n",
    "\n",
    "Now, I am going to analyse which season is more successful for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporada_sales_year_2018 = dataset[dataset.ANO_FACTURA == 2018].groupby(['TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "temporada_sales_year_2019 = dataset[dataset.ANO_FACTURA == 2019].groupby(['TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "temporada_sales_year_2020 = dataset[dataset.ANO_FACTURA == 2020].groupby(['TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "\n",
    "print('- 2018:\\n')\n",
    "for k, v in zip(temporada_sales_year_2018.index, temporada_sales_year_2018):\n",
    "    if k % 2 == 0:\n",
    "        print('Temporada Primavera-Verano: '+str(round(v,2))+'€')\n",
    "    else:\n",
    "        print('Temporada Otoño-Invierno: '+str(round(v,2))+'€')\n",
    "print('\\n- 2019:\\n')\n",
    "for k, v in zip(temporada_sales_year_2019.index, temporada_sales_year_2019):\n",
    "    if k % 2 == 0:\n",
    "        print('Temporada Primavera-Verano: '+str(round(v,2))+'€')\n",
    "    else:\n",
    "        print('Temporada Otoño-Invierno: '+str(round(v,2))+'€')\n",
    "print('\\n- 2020:\\n')\n",
    "for k, v in zip(temporada_sales_year_2020.index, temporada_sales_year_2020):\n",
    "    if k % 2 == 0:\n",
    "        print('Temporada Primavera-Verano: '+str(round(v,2))+'€')\n",
    "    else:\n",
    "        print('Temporada Otoño-Invierno: '+str(round(v,2))+'€')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commercial season in Camper has two different categories, Autum - Winter (odd numbers) and Spring - Summer (pair numbers). So every year, I will find three different codifications for each season.\n",
    "- In 2018:\n",
    "    - 85\n",
    "    - 86\n",
    "    - 87\n",
    "- In 2019\n",
    "    - 87\n",
    "    - 88\n",
    "    - 89\n",
    "- In 2020:\n",
    "    - 89\n",
    "    - 90\n",
    "    - 91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to do the representation of the total sales by month considering each season for the years 2018, 2019 and 2020.\n",
    "- 85 -> red, 86 -> blue, 87 -> green (2018)\n",
    "- 87 -> red, 88 -> blue, 89 -> green (2019)\n",
    "- 89 -> red, 90 -> blue, 91 -> green (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporada_sales_month_2018 = dataset[dataset.ANO_FACTURA == 2018].groupby(['MES_FACTURA', 'TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "months = []\n",
    "sales = []\n",
    "season = []\n",
    "for k, v in zip(temporada_sales_month_2018.index, temporada_sales_month_2018):\n",
    "    if v != 0:\n",
    "        months.append(k[0])\n",
    "        season.append(k[1])\n",
    "        sales.append(v)\n",
    "season_month_sales_2018 = pd.DataFrame({\n",
    "    'Month': months,\n",
    "    'Season': season,\n",
    "    'Money': sales\n",
    "})\n",
    "name_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.barh(name_months, season_month_sales_2018.Money, color= season_month_sales_2018['Season'].map({85: 'red', 86: 'blue', 87: 'green'}))\n",
    "plt.title('Sold in 2018')\n",
    "plt.savefig('../Output/seasonmonths2018.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporada_sales_month_2019 = dataset[dataset.ANO_FACTURA == 2019].groupby(['MES_FACTURA', 'TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "months = []\n",
    "sales = []\n",
    "season = []\n",
    "for k, v in zip(temporada_sales_month_2019.index, temporada_sales_month_2019):\n",
    "    if v != 0:\n",
    "        months.append(k[0])\n",
    "        season.append(k[1])\n",
    "        sales.append(v)\n",
    "season_month_sales_2019 = pd.DataFrame({\n",
    "    'Month': months,\n",
    "    'Season': season,\n",
    "    'Money': sales\n",
    "})\n",
    "name_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.barh(name_months, season_month_sales_2019.Money, color= season_month_sales_2019['Season'].map({87: 'red', 88: 'blue', 89: 'green'}))\n",
    "plt.title('Sold in 2019')\n",
    "plt.savefig('../Output/seasonmonths2019.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporada_sales_month_2020 = dataset[dataset.ANO_FACTURA == 2020].groupby(['MES_FACTURA', 'TEMPORADA_COMERCIAL_ID']).IMP_VENTA_NETO_EUR.sum()\n",
    "months = []\n",
    "sales = []\n",
    "season = []\n",
    "for k, v in zip(temporada_sales_month_2020.index, temporada_sales_month_2020):\n",
    "    if v != 0:\n",
    "        months.append(k[0])\n",
    "        season.append(k[1])\n",
    "        sales.append(v)\n",
    "season_month_sales_2020 = pd.DataFrame({\n",
    "    'Month': months,\n",
    "    'Season': season,\n",
    "    'Money': sales\n",
    "})\n",
    "name_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.barh(name_months, season_month_sales_2020.Money, color= season_month_sales_2020['Season'].map({89: 'red', 90: 'blue', 91: 'green'}))\n",
    "plt.title('Sold in 2020')\n",
    "plt.savefig('../Output/seasonmonths2020.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's analyse the column called TALLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDf(dt):\n",
    "    sales = []\n",
    "    size = []\n",
    "    for k, v in zip(dt.index, dt):\n",
    "        if v != 0:\n",
    "            size.append(k)\n",
    "            sales.append(v)\n",
    "    size_sales = pd.DataFrame({\n",
    "    'Size': size,\n",
    "    'Money': sales\n",
    "    })\n",
    "    return size_sales\n",
    "size_sales_2018 = dataset[dataset.ANO_FACTURA == 2018].groupby(['TALLA']).IMP_VENTA_NETO_EUR.sum()\n",
    "size_sales_2019 = dataset[dataset.ANO_FACTURA == 2019].groupby(['TALLA']).IMP_VENTA_NETO_EUR.sum()\n",
    "size_sales_2020 = dataset[dataset.ANO_FACTURA == 2020].groupby(['TALLA']).IMP_VENTA_NETO_EUR.sum()\n",
    "size_sales_2018 = prepareDf(size_sales_2018)\n",
    "size_sales_2019 = prepareDf(size_sales_2019)\n",
    "\n",
    "sales = []\n",
    "size = []\n",
    "for k, v in zip(size_sales_2020.index, size_sales_2020): \n",
    "    if v != 0 and k not in size:\n",
    "        try:\n",
    "            k = int(k)\n",
    "        except:\n",
    "            k = k\n",
    "        if k in size:\n",
    "            i = size.index(k)\n",
    "            sales[i] = v + sales[i]\n",
    "        else:\n",
    "            try:\n",
    "                size.append(int(k))\n",
    "            except:\n",
    "                size.append(k)\n",
    "            sales.append(v)\n",
    "size_sales_2020 = pd.DataFrame({\n",
    "    'Size': size,\n",
    "    'Money': sales\n",
    "})\n",
    "\n",
    "plt.bar(size_sales_2018.index, size_sales_2018.Money, alpha=0.2)\n",
    "plt.title('Size (2018)')\n",
    "plt.tick_params(axis='x', which='major', labelsize=4)\n",
    "plt.xticks(size_sales_2018.index, size_sales_2018.Size, fontweight='bold')\n",
    "plt.savefig('../Output/sizesales2018.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(size_sales_2019.index, size_sales_2019.Money, alpha=0.2)\n",
    "plt.title('Size (2019)')\n",
    "plt.tick_params(axis='x', which='major', labelsize=4)\n",
    "plt.xticks(size_sales_2019.index, size_sales_2019.Size, fontweight='bold')\n",
    "plt.savefig('../Output/sizesales2019.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(size_sales_2020.index, size_sales_2020.Money, alpha=0.2)\n",
    "plt.title('Size (2020)')\n",
    "plt.tick_params(axis='x', which='major', labelsize=4)\n",
    "plt.xticks(size_sales_2020.index, size_sales_2020.Size, fontweight='bold')\n",
    "plt.savefig('../Output/sizesales2020.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a the graphs, it can be seen that for each year I have similirar distribution being a Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to see the distribution of the data grouped by the genre. Same as before, I haven't treated the null variables yet so I am not considering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareGenresales(df):\n",
    "    genre = []\n",
    "    sales = []\n",
    "    for k, v in zip(df.index, df):\n",
    "        genre.append(k)\n",
    "        sales.append(v)\n",
    "    genre_sales_year = pd.DataFrame({\n",
    "        'Genero': genre,\n",
    "        'Money': sales\n",
    "    })\n",
    "    return genre_sales_year\n",
    "genre_sales_year_2018 = dataset[dataset.ANO_FACTURA == 2018].groupby(['GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "genre_sales_year_2019 = dataset[dataset.ANO_FACTURA == 2019].groupby(['GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "genre_sales_year_2020 = dataset[dataset.ANO_FACTURA == 2020].groupby(['GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "\n",
    "genre_sales_year_2018 = prepareGenresales(genre_sales_year_2018)\n",
    "genre_sales_year_2019 =  prepareGenresales(genre_sales_year_2019)\n",
    "genre_sales_year_2020 = prepareGenresales(genre_sales_year_2020)\n",
    "\n",
    "plt.pie(genre_sales_year_2018.Money, explode=(0.05, 0.05, 0.05, 0.05), labels= genre_sales_year_2018.Genero, autopct='%1.2f%%', shadow= True, startangle= 90)\n",
    "plt.title('Product genre (2018)')\n",
    "plt.savefig('../Output/productgen2018.png')\n",
    "plt.show()\n",
    "\n",
    "plt.pie(genre_sales_year_2019.Money, explode=(0.05, 0.05, 0.05, 0.05), labels= genre_sales_year_2019.Genero, autopct='%1.2f%%', shadow= True, startangle= 90)\n",
    "plt.title('Product genre (2019)')\n",
    "plt.savefig('../Output/productgen2019.png')\n",
    "plt.show()\n",
    "\n",
    "plt.pie(genre_sales_year_2020.Money, explode=(0.05, 0.05, 0.05, 0.05), labels= genre_sales_year_2020.Genero, autopct='%1.2f%%', shadow= True, startangle= 90)\n",
    "plt.title('Product genre (2020)')\n",
    "plt.savefig('../Output/productgen2020.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the pie chart, women are the most sold, with an almost the 50%, men are the next one with 43%. The genre kids is the worst in terms of sells. The market should be oriented in men and women. Also, they can extract from the data that one open market to exploit is Kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparegenreseason(df):\n",
    "    genre = []\n",
    "    seasons = []\n",
    "    sales = []\n",
    "    for k, v in zip(df.index, df):\n",
    "        seasons.append(k[0])\n",
    "        genre.append(k[1])\n",
    "        sales.append(v)\n",
    "    genre_sales_year = pd.DataFrame({\n",
    "        'Season': seasons,\n",
    "        'Genero': genre,\n",
    "        'Money': sales\n",
    "    })\n",
    "    return genre_sales_year\n",
    "\n",
    "def plotting(df, name, seasn):\n",
    "    plt.bar([0, 1, 2, 3], df[df.Season == seasn[0]].Money, color= 'red', edgecolor='white', width=0.5)\n",
    "    plt.bar([0, 1, 2, 3], df[df.Season == seasn[1]].Money, color= 'blue', bottom=df[df.Season == seasn[0]].Money, edgecolor='white', width=0.5)\n",
    "    plt.bar([0, 1, 2, 3], df[df.Season == seasn[2]].Money, color= 'green', bottom=df[df.Season == seasn[1]].Money, edgecolor='white', width=0.5)\n",
    "    plt.xticks([0, 1, 2, 3], ['KIDS', 'MEN', 'NV','WOMEN'], fontweight='bold')\n",
    "    plt.title(name)\n",
    "    plt.legend(seasn)\n",
    "    plt.savefig('../Output/'+name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "genre_season_sales_2018 = dataset[dataset.ANO_FACTURA == 2018].groupby(['TEMPORADA_COMERCIAL_ID', 'GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "genre_season_sales_2019 = dataset[dataset.ANO_FACTURA == 2019].groupby(['TEMPORADA_COMERCIAL_ID', 'GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "genre_season_sales_2020 = dataset[dataset.ANO_FACTURA == 2020].groupby(['TEMPORADA_COMERCIAL_ID', 'GENERO_PRODUCTO']).IMP_VENTA_NETO_EUR.sum()\n",
    "genre_season_sales_2018 = preparegenreseason(genre_season_sales_2018)\n",
    "genre_season_sales_2019 = preparegenreseason(genre_season_sales_2019)\n",
    "genre_season_sales_2020 = preparegenreseason(genre_season_sales_2020)\n",
    "\n",
    "plotting(genre_season_sales_2018, 'Season - Genre Product (2018)', [85, 86, 87])\n",
    "plotting(genre_season_sales_2019, 'Season - Genre Product (2019)', [87, 88, 89])\n",
    "plotting(genre_season_sales_2020, 'Season - Genre Product (2020)', [89, 90, 91])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results. In all cases, the Autum - Winter season in the one with more contribution in each genre product category.\n",
    "\n",
    "The next step is calculate the average customer age, for doing this, I am going to consider only the informed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepagesales(df):\n",
    "    money = []\n",
    "    age_range = []\n",
    "    for k, v in zip(df.index, df):\n",
    "        money.append(v)\n",
    "        age_range.append(k)\n",
    "    age_grouped_sales = pd.DataFrame({\n",
    "        'Age_Range': age_range,\n",
    "        'Money': money\n",
    "    })\n",
    "    return age_grouped_sales\n",
    "def printagesales(df, name):\n",
    "    #, color=['red', 'blue', 'purple', 'green', 'lavender', 'orange', 'yellow', 'brown', 'gray']\n",
    "    plt.bar([0, 1, 2, 3, 4, 5, 6, 7], df.Money, width=0.5)\n",
    "    plt.xticks([0, 1, 2, 3, 4, 5, 6, 7], ['19 --','20-29','30-39','40-49','50-59','60-69','70 ++','NV'], fontweight='bold')\n",
    "    plt.title(name)\n",
    "    plt.savefig('../Output/'+name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "print('Average customer age (2018): '+str(round(dataset[(dataset.ANO_FACTURA == 2018) & (dataset.EDAD_SN == 'S')].EDAD_COMPRA.mean(), 0)))\n",
    "print('Average customer age (2019): '+str(round(dataset[(dataset.ANO_FACTURA == 2019) & (dataset.EDAD_SN == 'S')].EDAD_COMPRA.mean(), 0)))\n",
    "print('Average customer age (2020): '+str(round(dataset[(dataset.ANO_FACTURA == 2020) & (dataset.EDAD_SN == 'S')].EDAD_COMPRA.mean(), 0)))\n",
    "\n",
    "age_range_sales_2018 = prepagesales(dataset[(dataset.ANO_FACTURA == 2018)].groupby(['EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum())\n",
    "age_range_sales_2019 = prepagesales(dataset[(dataset.ANO_FACTURA == 2019)].groupby(['EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum())\n",
    "age_range_sales_2020 = prepagesales(dataset[(dataset.ANO_FACTURA == 2020)].groupby(['EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum())\n",
    "\n",
    "printagesales(age_range_sales_2018, 'Age range distribution (2018)')\n",
    "printagesales(age_range_sales_2018, 'Age range distribution (2019)')\n",
    "printagesales(age_range_sales_2018, 'Age range distribution (2020)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen in the images above, they have a Gaussian distribution, without considering the non informed values (NV), having their center at range 40-49. So with the data given and without paying attention to the NV', the market should be oriented for people between 30 to 59 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to analyse the sales across all the countries where Camper is selling products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_country_2018 = round(dataset[(dataset.ANO_FACTURA == 2018)].groupby(['NUMERO_DEUDOR_PAIS_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "total_sales_country_2019 = round(dataset[(dataset.ANO_FACTURA == 2019)].groupby(['NUMERO_DEUDOR_PAIS_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "total_sales_country_2020 = round(dataset[(dataset.ANO_FACTURA == 2020)].groupby(['NUMERO_DEUDOR_PAIS_ID']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "print('Top 10 countries 2018: \\n')\n",
    "display(total_sales_country_2018.sort_values(ascending = False).head(10))\n",
    "print('\\nTop 10 countries 2019: \\n')\n",
    "display(total_sales_country_2019.sort_values(ascending = False).head(10))\n",
    "print('\\nTop 10 countries 2020: \\n')\n",
    "display(total_sales_country_2020.sort_values(ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, data grouped by Country Genre_product      Product Age_range     Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareCGPA(df):\n",
    "    money = []\n",
    "    product = []\n",
    "    country=[]\n",
    "    genre_product = []\n",
    "    age_range = []\n",
    "    for k, v in zip(df.index, df):\n",
    "        money.append(v)\n",
    "        country.append(k[0])\n",
    "        genre_product.append(k[1])\n",
    "        product.append(k[2])\n",
    "        age_range.append(k[3])\n",
    "\n",
    "    country_genre_product_age = pd.DataFrame({\n",
    "        'Country': country,\n",
    "        'Genre_product': genre_product,\n",
    "        'Product': product,\n",
    "        'Age_range': age_range,\n",
    "        'Money': money\n",
    "    })\n",
    "    return country_genre_product_age\n",
    "\n",
    "country_genre_product_age_2018 = round(dataset[(dataset.ANO_FACTURA == 2018)].groupby(['NUMERO_DEUDOR_PAIS_ID', 'GENERO_PRODUCTO', 'PRODUCTO_ID', 'EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "country_genre_product_age_2019 = round(dataset[(dataset.ANO_FACTURA == 2019)].groupby(['NUMERO_DEUDOR_PAIS_ID', 'GENERO_PRODUCTO', 'PRODUCTO_ID', 'EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "country_genre_product_age_2020 = round(dataset[(dataset.ANO_FACTURA == 2020)].groupby(['NUMERO_DEUDOR_PAIS_ID', 'GENERO_PRODUCTO', 'PRODUCTO_ID', 'EDAD_RANGO_COMPRA']).IMP_VENTA_NETO_EUR.sum(), 2)\n",
    "\n",
    "country_genre_product_age_2018 = prepareCGPA(country_genre_product_age_2018)\n",
    "country_genre_product_age_2019 = prepareCGPA(country_genre_product_age_2019)\n",
    "country_genre_product_age_2020 = prepareCGPA(country_genre_product_age_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 genre product sold in countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%INCLUDE MOST GENRE SOLD FOR EACH COUNTRY%\n",
    "display(country_genre_product_age_2018.groupby(['Country', 'Genre_product']).Money.sum().sort_values(ascending= False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2018:\\n')\n",
    "display(country_genre_product_age_2018.sort_values( by = ['Money'], ascending = False).head(10))\n",
    "print('\\n 2019:\\n')\n",
    "display(country_genre_product_age_2018.sort_values( by = ['Money'], ascending = False).head(10))\n",
    "print('\\n 2020:\\n')\n",
    "display(country_genre_product_age_2018.sort_values( by = ['Money'], ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_analysis_2018 = dataset[(dataset.ANO_FACTURA == 2018)].groupby(['IDIOMA_CONTACTO']).agg({'FACTURA_ID':'count'}).reset_index().rename(columns= {'FACTURA_ID': 'Count'})\n",
    "language_analysis_2019 = dataset[(dataset.ANO_FACTURA == 2019)].groupby(['IDIOMA_CONTACTO']).agg({'FACTURA_ID':'count'}).reset_index().rename(columns= {'FACTURA_ID': 'Count'})\n",
    "language_analysis_2020 = dataset[(dataset.ANO_FACTURA == 2020)].groupby(['IDIOMA_CONTACTO']).agg({'FACTURA_ID':'count'}).reset_index().rename(columns= {'FACTURA_ID': 'Count'})\n",
    "lanan2018 = language_analysis_2018[language_analysis_2018.IDIOMA_CONTACTO == 'NV']['Count']\n",
    "lanan2019 = language_analysis_2019[language_analysis_2019.IDIOMA_CONTACTO == 'NV']['Count']\n",
    "lanan2020 = language_analysis_2020[language_analysis_2020.IDIOMA_CONTACTO == 'NV']['Count']\n",
    "print('Language not informed (2018): '+lanan2018.to_string(index=False)+ '\\t'+(lanan2018/language_analysis_2018['Count'].sum()*100).to_string(index=False)+'%')\n",
    "print('Language not informed (2019): '+lanan2019.to_string(index=False)+ '\\t'+(lanan2018/language_analysis_2019['Count'].sum()*100).to_string(index=False)+'%')\n",
    "print('Language not informed (2020): '+lanan2020.to_string(index=False)+ '\\t'+(lanan2018/language_analysis_2020['Count'].sum()*100).to_string(index=False)+'%')\n",
    "print('\\nTop 3 most used language (2018):')\n",
    "display(language_analysis_2018[language_analysis_2018.IDIOMA_CONTACTO != 'NV'].sort_values(by=['Count'] ,ascending = False).head(3))\n",
    "print('\\nTop 3 most used language (2019):')\n",
    "display(language_analysis_2019[language_analysis_2019.IDIOMA_CONTACTO != 'NV'].sort_values(by=['Count'] ,ascending = False).head(3))\n",
    "print('\\nTop 3 most used language (2020):')\n",
    "display(language_analysis_2020[language_analysis_2020.IDIOMA_CONTACTO != 'NV'].sort_values(by=['Count'] ,ascending = False).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, each year the most used languages are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como que lo que quiero es ver las variables que me interesan para el modelo, solo uso las ventas, el modelo tiene que predecir las ventas de un producto y una talla\n",
    "pcadataset = dataset[dataset.FACTURA_CLASE_DOCUMENTO_ID == 'ZTON']\n",
    "pcadataset = pcadataset.drop(columns=['FACTURA_ID', 'FACTURA_POSICION_ID', 'CUSTOMER_ID', 'FACTURA_CLASE_DOCUMENTO_ID', 'COLOR', 'CANAL_VENTA_ID', 'VENTA_DEVOLUCION'])\n",
    "sdt = pcadataset.sort_values(['ANO_FACTURA', 'MES_FACTURA', 'FECHA_FACTURA'])\n",
    "sdt.to_csv('info_data.csv', index = False, header=True) # Export raw data\n",
    "display(sdt.shape)\n",
    "print(pcadataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 2-values labels\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "pcadataset['CONTACTO_SN'].replace(to_replace= 'S', value = 1, inplace = True)\n",
    "pcadataset['CONTACTO_SN'].replace(to_replace= 'N', value = 0, inplace = True)\n",
    "pcadataset['EDAD_SN'].replace(to_replace= 'S', value = 1, inplace = True)\n",
    "pcadataset['EDAD_SN'].replace(to_replace= 'N', value = 0, inplace = True)\n",
    "pcadataset[\"GENERO_PRODUCTO\"] = label_encoder.fit_transform(pcadataset[[\"GENERO_PRODUCTO\"]])\n",
    "pcadataset[\"GENERO_CONTACTO\"] = label_encoder.fit_transform(pcadataset[[\"GENERO_CONTACTO\"]])\n",
    "pcadataset[\"TALLA\"].replace(to_replace = 'NV', value = 0, inplace = True)\n",
    "#pcadataset[\"TALLA\"] = label_encoder.fit_transform(pcadataset[[\"TALLA\"]])\n",
    "pcadataset[\"PRODUCTO_ID\"] = label_encoder.fit_transform(pcadataset[[\"PRODUCTO_ID\"]])\n",
    "pcadataset[\"ESFUERZO_VENTA_ID\"].replace(to_replace = 'NV', value = 0, inplace = True)\n",
    "pcadataset[\"ESFUERZO_VENTA_ID\"] = pcadataset[\"ESFUERZO_VENTA_ID\"].astype(str)\n",
    "pcadataset[\"ESFUERZO_VENTA_ID\"] = label_encoder.fit_transform(pcadataset[[\"ESFUERZO_VENTA_ID\"]])\n",
    "pcadataset[\"JERARQUIA_PROD_ID\"] = label_encoder.fit_transform(pcadataset[[\"JERARQUIA_PROD_ID\"]])\n",
    "pcadataset[\"CATEGORIA\"] = label_encoder.fit_transform(pcadataset[[\"CATEGORIA\"]])\n",
    "pcadataset[\"TIPOLOGIA\"] = label_encoder.fit_transform(pcadataset[[\"TIPOLOGIA\"]])\n",
    "pcadataset[\"CONSUMER_COLOR\"] = label_encoder.fit_transform(pcadataset[[\"CONSUMER_COLOR\"]])\n",
    "pcadataset[\"CIUDAD_CONTACTO\"] = label_encoder.fit_transform(pcadataset[[\"CIUDAD_CONTACTO\"]])\n",
    "pcadataset[\"IDIOMA_CONTACTO\"] = label_encoder.fit_transform(pcadataset[[\"IDIOMA_CONTACTO\"]])\n",
    "pcadataset[\"CREMALLERA\"] = label_encoder.fit_transform(pcadataset[[\"CREMALLERA\"]])\n",
    "pcadataset[\"CORDONES\"] = label_encoder.fit_transform(pcadataset[[\"CORDONES\"]])\n",
    "pcadataset[\"OUTSOLE_SUELA_TIPO\"] = label_encoder.fit_transform(pcadataset[[\"OUTSOLE_SUELA_TIPO\"]])\n",
    "pcadataset[\"OUTSOLE_SUELA_SUBTIPO\"] = label_encoder.fit_transform(pcadataset[[\"OUTSOLE_SUELA_SUBTIPO\"]])\n",
    "pcadataset[\"PLANTILLA_EXTRAIBLE\"] = label_encoder.fit_transform(pcadataset[[\"PLANTILLA_EXTRAIBLE\"]])\n",
    "pcadataset[\"NUMERO_DEUDOR_PAIS_ID\"] = label_encoder.fit_transform(pcadataset[[\"NUMERO_DEUDOR_PAIS_ID\"]])\n",
    "pcadataset.to_csv('info_datav2.csv', index = False, header=True) # Export data labelled\n",
    "pcadataset = pcadataset.drop(columns=['EDAD_RANGO_COMPRA', 'PRODUCTO_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl = {'IL': 972, 'Jerusalem': 9722, 'IE': 353, 'Dublin 8': 3531, 'CA': 1, 'Vancouver': 1250, 'AT': 43, 'Perchtoldsdorf': 431, 'Wien': 431, 'PL': 48, '?omianki': 4822, 'Bojano': 4822, 'CH': 41, 'Thalwil': 4143, 'Z?rich': 4143, 'Bottighofen': 4143, 'Dinhard': 4150, 'Allschwil ': 4123, 'Glarus': 4155, 'Basel': 4161, 'Eglisau ': 4143, 'Chavannes-pr?s-Renens': 4122, 'Baden': 4156, 'Wettingen': 4130, 'Corpataux': 4126, 'Tramelan': 4132, 'Les Paccots': 4119, 'Greifensee': 4144, 'RO': 40, 'Bucharest': 4021, 'Sementina': 4114, 'Baar': 4141, 'NV': 4100, 'winterthur': 4152, 'Geneva': 4122, 'Ursy': 4100, 'Satigny': 4142, 'Lausanne': 4121, 'Amriswil': 4171, 'bern': 4131, 'D?bendorf ': 4144, 'D?rnten': 4155, 'Yens': 4121, 'Auvernier': 4101, 'Lutry': 4195, 'Monthey': 4124, 'Bulle': 4126, 'Grosswangen': 4190, 'Bellinzona': 4191, 'Vevey': 4121, 'Ueken': 4162, 'Netstal': 4190, 'Fribourg': 4126, 'Saillon': 4190, 'Thun': 4133, 'Niederrohrdorf': 4156, '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': , '': ,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric data\n",
    "scaler = StandardScaler()\n",
    "scaled = pd.DataFrame(scaler.fit_transform(pcadataset), columns = pcadataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Proportion of Variance Explained : ', pca.explained_variance_ratio_)  \n",
    "print('')\n",
    "out_sum = np.cumsum(pca.explained_variance_ratio_)  \n",
    "print ('Cumulative Prop. Variance Explained: ', out_sum)\n",
    "print('')\n",
    "print('Explained variance: ', pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_\n",
    "num_pc = pca.n_features_\n",
    "pc_list = [\"PC\"+str(i) for i in list(range(1, num_pc+1))]\n",
    "loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\n",
    "loadings_df['variable'] = pcadataset.columns.values\n",
    "loadings_df = loadings_df.set_index('variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(loadings_df, annot=False, cmap='Spectral', xticklabels=True, yticklabels=True)\n",
    "plt.tick_params(axis='x', which='major', labelsize=4)\n",
    "plt.tick_params(axis='y', which='major', labelsize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.bar(PC_values, pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.pcaplot(x=loadings[0], y=loadings[1], labels=pcadataset.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2), var2=round(pca.explained_variance_ratio_[1]*100, 2), show= True)\n",
    "cluster.pcaplot(x=loadings[0], y=loadings[1], labels=pcadataset.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2), var2=round(pca.explained_variance_ratio_[1]*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = label_encoder.fit_transform(dataset[dataset.FACTURA_CLASE_DOCUMENTO_ID == 'ZTON'][\"PRODUCTO_ID\"])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1],c=colors, cmap='rainbow')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.savefig('PCA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = label_encoder.fit_transform(dataset[dataset.FACTURA_CLASE_DOCUMENTO_ID == 'ZTON'][\"PRODUCTO_ID\"])\n",
    "#cluster.biplot(cscore=principalComponents, loadings=loadings, labels=pcadataset.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2),\n",
    "#var2=round(pca.explained_variance_ratio_[1]*100, 2), show= True)\n",
    "cluster.biplot(cscore=principalComponents, loadings=loadings, labels=pcadataset.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2),\n",
    "var2=round(pca.explained_variance_ratio_[1]*100, 2), show= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdeu = dataset[dataset.FACTURA_CLASE_DOCUMENTO_ID == 'ZTON'][['NUMERO_DEUDOR_PAIS_ID', 'CIUDAD_CONTACTO']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
